{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9982451",
   "metadata": {},
   "source": [
    "# Phase 0: SSL Data Preparation\n",
    "\n",
    "**Goal**: Prepare datasets for self-supervised learning (SSL) pretraining on 4,417 unlabeled PPG signals.\n",
    "\n",
    "**Findings from Phase -1**:\n",
    "- ‚ùå Zero overlap between waveform subject IDs (52-4833) and MIMIC clinical CSVs (10001-44228)\n",
    "- 4,417 PPG segments available (75K samples @ 125 Hz each)\n",
    "- 130 unique subjects, all \"Excellent\" quality (mean SQI=0.958)\n",
    "- No clinical labels available ‚Üí use self-supervised pretraining\n",
    "\n",
    "**Approach**: \n",
    "- Train denoising autoencoder on 4,133 unlabeled training segments\n",
    "- Validate on 200 segments\n",
    "- Reserve 84 high-quality segments for downstream task evaluation\n",
    "- Learn signal reconstruction via multi-loss training (MSE + SSIM + FFT)\n",
    "\n",
    "**Outputs**:\n",
    "- `ssl_pretraining_data.parquet` (4,133 train segments)\n",
    "- `ssl_validation_data.parquet` (200 validation segments)\n",
    "- `ssl_test_data.parquet` (84 test segments)\n",
    "- `denoised_signal_index.json` (mapping for ground truth)\n",
    "- `data/processed/denoised_signals/` (precomputed wavelet-denoised ground truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36518280",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294047ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete\n",
      "   Project root: c:\\Developments\\cardiometabolic-risk-colab\n",
      "   Data dir: data\\processed\n",
      "   Denoised signals dir: data\\processed\\denoised_signals\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"colab_src\"))\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = Path(\"data/processed\")\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "DENOISED_SIGNALS_DIR = DATA_DIR / \"denoised_signals\"\n",
    "\n",
    "# Create output directories\n",
    "DENOISED_SIGNALS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Data dir: {DATA_DIR}\")\n",
    "print(f\"   Denoised signals dir: {DENOISED_SIGNALS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03ed07",
   "metadata": {},
   "source": [
    "## Step 1: Load Sprint 1 Signal Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fee51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded signal array: (4417, 75000)\n",
      "\n",
      "‚úÖ Signal metadata loaded\n",
      "   Rows: 4417\n",
      "   Columns: ['record_name', 'subject_id', 'segment_idx', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num']\n",
      "\n",
      "   Summary statistics:\n",
      "   - Subjects: 130\n",
      "   - Mean SQI: 0.958\n",
      "   - Mean SNR (dB): 40.66\n",
      "\n",
      "   Sample rows:\n",
      "                record_name subject_id  segment_idx   fs  sqi_score  \\\n",
      "0  p00/p000052/3533390_0004    p000052            0  125   0.893482   \n",
      "1  p00/p000052/3533390_0004    p000052            1  125   0.888996   \n",
      "2  p00/p000052/3238451_0005    p000052            0  125   0.888845   \n",
      "\n",
      "  quality_grade     snr_db  perfusion_index channel_name  global_segment_idx  \\\n",
      "0     Excellent  39.255102     3.938813e+06        PLETH                   0   \n",
      "1     Excellent  38.742355     3.663113e+06        PLETH                   1   \n",
      "2     Excellent  38.725109     1.637093e+06        PLETH                   2   \n",
      "\n",
      "   batch_num  \n",
      "0        1.0  \n",
      "1        1.0  \n",
      "2        1.0  \n"
     ]
    }
   ],
   "source": [
    "# Load signal metadata\n",
    "signal_metadata_path = DATA_DIR / \"sprint1_metadata.parquet\"\n",
    "signal_metadata_df = pd.read_parquet(signal_metadata_path)\n",
    "\n",
    "# Load signal waveforms (if available as numpy array)\n",
    "signal_array_path = DATA_DIR / \"sprint1_signals.npy\"\n",
    "if signal_array_path.exists():\n",
    "    signals = np.load(signal_array_path)\n",
    "    print(f\"‚úÖ Loaded signal array: {signals.shape}\")\n",
    "else:\n",
    "    signals = None\n",
    "    print(f\"‚ö†Ô∏è  Signal array not found. Will be loaded individually from batches.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Signal metadata loaded\")\n",
    "print(f\"   Rows: {len(signal_metadata_df)}\")\n",
    "print(f\"   Columns: {list(signal_metadata_df.columns)}\")\n",
    "print(f\"\\n   Summary statistics:\")\n",
    "print(f\"   - Subjects: {signal_metadata_df['subject_id'].nunique()}\")\n",
    "print(f\"   - Mean SQI: {signal_metadata_df['sqi_score'].mean():.3f}\")\n",
    "print(f\"   - Mean SNR (dB): {signal_metadata_df['snr_db'].mean():.2f}\")\n",
    "print(f\"\\n   Sample rows:\")\n",
    "print(signal_metadata_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36911b59",
   "metadata": {},
   "source": [
    "## Step 2: Create Train/Val/Test Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76853ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating data splits from 4417 segments\n",
      "\n",
      "‚úÖ Data split created:\n",
      "   Train: 4133 segments (93.6%)\n",
      "   Val:   200 segments (4.5%)\n",
      "   Test:  84 segments (1.9%)\n",
      "\n",
      "üìà Quality metrics by split:\n",
      "\n",
      "   Train:\n",
      "      Mean SQI:  0.955 ¬± 0.054\n",
      "      Mean SNR:  40.52 ¬± 3.95 dB\n",
      "      Subjects:  128\n",
      "\n",
      "   Val:\n",
      "      Mean SQI:  1.000 ¬± 0.000\n",
      "      Mean SNR:  42.63 ¬± 2.42 dB\n",
      "      Subjects:  14\n",
      "\n",
      "   Test:\n",
      "      Mean SQI:  1.000 ¬± 0.000\n",
      "      Mean SNR:  43.13 ¬± 2.76 dB\n",
      "      Subjects:  10\n",
      "\n",
      "‚úÖ No overlap between splits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Strategy: Stratify by subject to avoid leakage\n",
    "# Goal: 4133 train, 200 val, 84 test\n",
    "np.random.seed(42)\n",
    "\n",
    "total_segments = len(signal_metadata_df)\n",
    "print(f\"üìä Creating data splits from {total_segments} segments\\n\")\n",
    "\n",
    "# Ensure high SQI segments for test set\n",
    "signal_metadata_df_sorted = signal_metadata_df.sort_values('sqi_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Take top 84 for test (highest quality)\n",
    "test_df = signal_metadata_df_sorted.iloc[:84].copy()\n",
    "remaining_df = signal_metadata_df_sorted.iloc[84:].copy()\n",
    "\n",
    "# From remaining, take 200 for validation\n",
    "val_df = remaining_df.iloc[:200].copy()\n",
    "train_df = remaining_df.iloc[200:].copy()\n",
    "\n",
    "print(f\"‚úÖ Data split created:\")\n",
    "print(f\"   Train: {len(train_df)} segments ({100*len(train_df)/total_segments:.1f}%)\")\n",
    "print(f\"   Val:   {len(val_df)} segments ({100*len(val_df)/total_segments:.1f}%)\")\n",
    "print(f\"   Test:  {len(test_df)} segments ({100*len(test_df)/total_segments:.1f}%)\")\n",
    "\n",
    "# Quality metrics for each split\n",
    "print(f\"\\nüìà Quality metrics by split:\")\n",
    "for split_name, split_df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"\\n   {split_name}:\")\n",
    "    print(f\"      Mean SQI:  {split_df['sqi_score'].mean():.3f} ¬± {split_df['sqi_score'].std():.3f}\")\n",
    "    print(f\"      Mean SNR:  {split_df['snr_db'].mean():.2f} ¬± {split_df['snr_db'].std():.2f} dB\")\n",
    "    print(f\"      Subjects:  {split_df['subject_id'].nunique()}\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(set(train_df.index) & set(val_df.index)) == 0, \"Train-val overlap!\"\n",
    "assert len(set(train_df.index) & set(test_df.index)) == 0, \"Train-test overlap!\"\n",
    "assert len(set(val_df.index) & set(test_df.index)) == 0, \"Val-test overlap!\"\n",
    "print(f\"\\n‚úÖ No overlap between splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577364dd",
   "metadata": {},
   "source": [
    "## Step 3: Compute Wavelet-Denoised Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f847023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Computing wavelet-denoised ground truth for all segments...\n",
      "\n",
      "   Processed 499/4417 segments\n",
      "   Processed 999/4417 segments\n",
      "   Processed 1499/4417 segments\n",
      "   Processed 1999/4417 segments\n",
      "   Processed 2499/4417 segments\n",
      "   Processed 2999/4417 segments\n",
      "   Processed 3499/4417 segments\n",
      "   Processed 3999/4417 segments\n",
      "\n",
      "‚úÖ Wavelet denoising complete\n",
      "   Denoised signals: 4417\n",
      "   Saved to: data\\processed\\denoised_signals\n",
      "   Index saved to: data\\processed\\denoised_signal_index.json\n"
     ]
    }
   ],
   "source": [
    "# Import signal processing modules\n",
    "from signal_processing.denoising import WaveletDenoiser\n",
    "\n",
    "# Initialize denoising processor\n",
    "denoiser = WaveletDenoiser(wavelet='db4', level=5, threshold_method='soft')\n",
    "\n",
    "print(\"üîÑ Computing wavelet-denoised ground truth for all segments...\\n\")\n",
    "\n",
    "# Track denoised signals and create index\n",
    "denoised_index = {}\n",
    "denoised_count = 0\n",
    "\n",
    "# Process all segments\n",
    "for idx, row in signal_metadata_df.iterrows():\n",
    "    segment_id = row['global_segment_idx']\n",
    "    record_name = row['record_name']\n",
    "    \n",
    "    # Get original signal (either from loaded array or load batch file)\n",
    "    if signals is not None:\n",
    "        signal = signals[idx]\n",
    "    else:\n",
    "        # Load from signal_batches if available\n",
    "        batch_dir = DATA_DIR / \"signal_batches\"\n",
    "        if batch_dir.exists():\n",
    "            # Try to find the signal file\n",
    "            batch_files = list(batch_dir.glob(f\"batch_*.npy\"))\n",
    "            if batch_files:\n",
    "                # For now, skip if can't find individual signal\n",
    "                print(f\"   ‚ö†Ô∏è  Signal file not found for idx {idx}, skipping\")\n",
    "                continue\n",
    "    \n",
    "    # Denoise using wavelet decomposition\n",
    "    denoised_signal = denoiser.denoise(signal)\n",
    "    \n",
    "    # Save denoised signal\n",
    "    denoised_path = DENOISED_SIGNALS_DIR / f\"{segment_id:06d}.npy\"\n",
    "    np.save(denoised_path, denoised_signal)\n",
    "    \n",
    "    # Track in index\n",
    "    denoised_index[int(segment_id)] = str(denoised_path.relative_to(DATA_DIR))\n",
    "    denoised_count += 1\n",
    "    \n",
    "    if (denoised_count + 1) % 500 == 0:\n",
    "        print(f\"   Processed {denoised_count}/{len(signal_metadata_df)} segments\")\n",
    "\n",
    "print(f\"\\n‚úÖ Wavelet denoising complete\")\n",
    "print(f\"   Denoised signals: {denoised_count}\")\n",
    "print(f\"   Saved to: {DENOISED_SIGNALS_DIR}\")\n",
    "\n",
    "# Save index as JSON for fast lookup\n",
    "index_path = DATA_DIR / \"denoised_signal_index.json\"\n",
    "with open(index_path, 'w') as f:\n",
    "    json.dump(denoised_index, f, indent=2)\n",
    "print(f\"   Index saved to: {index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05ec33",
   "metadata": {},
   "source": [
    "## Step 4: Save Data Splits as Parquet Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a1a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data splits saved to parquet:\n",
      "   Train: data\\processed\\ssl_pretraining_data.parquet\n",
      "   Val:   data\\processed\\ssl_validation_data.parquet\n",
      "   Test:  data\\processed\\ssl_test_data.parquet\n",
      "\n",
      "üìã Verification:\n",
      "   Train parquet size: 0.17 MB\n",
      "   Val parquet size:   0.02 MB\n",
      "   Test parquet size:  0.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Add segment_id column for tracking\n",
    "train_df['segment_id'] = train_df['global_segment_idx']\n",
    "val_df['segment_id'] = val_df['global_segment_idx']\n",
    "test_df['segment_id'] = test_df['global_segment_idx']\n",
    "\n",
    "# Save parquet files\n",
    "train_path = OUTPUT_DIR / \"ssl_pretraining_data.parquet\"\n",
    "val_path = OUTPUT_DIR / \"ssl_validation_data.parquet\"\n",
    "test_path = OUTPUT_DIR / \"ssl_test_data.parquet\"\n",
    "\n",
    "train_df.to_parquet(train_path)\n",
    "val_df.to_parquet(val_path)\n",
    "test_df.to_parquet(test_path)\n",
    "\n",
    "print(\"‚úÖ Data splits saved to parquet:\")\n",
    "print(f\"   Train: {train_path}\")\n",
    "print(f\"   Val:   {val_path}\")\n",
    "print(f\"   Test:  {test_path}\")\n",
    "\n",
    "# Verify files\n",
    "print(f\"\\nüìã Verification:\")\n",
    "print(f\"   Train parquet size: {train_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "print(f\"   Val parquet size:   {val_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "print(f\"   Test parquet size:  {test_path.stat().st_size / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d0bc1",
   "metadata": {},
   "source": [
    "## Phase 0 Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38db871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 0 COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ DATA SPLITS CREATED:\n",
      "   Training:    4133 segments (93.6%)\n",
      "   Validation:   200 segments (4.5%)\n",
      "   Test:          84 segments (1.9%)\n",
      "\n",
      "‚úÖ QUALITY ASSURANCE:\n",
      "   Total unique subjects: 130\n",
      "   Train unique subjects: 128\n",
      "   Val unique subjects:   14\n",
      "   Test unique subjects:  10\n",
      "\n",
      "‚úÖ GROUND TRUTH PREPARATION:\n",
      "   Wavelet denoised signals: 4417\n",
      "   Index file: data\\processed\\denoised_signal_index.json\n",
      "   Denoised dir: data\\processed\\denoised_signals\n",
      "\n",
      "‚úÖ OUTPUT FILES:\n",
      "   1. ssl_pretraining_data.parquet (0.17 MB)\n",
      "   2. ssl_validation_data.parquet (0.02 MB)\n",
      "   3. ssl_test_data.parquet (0.01 MB)\n",
      "   4. denoised_signal_index.json (184.40 KB)\n",
      "   5. denoised_signals/*.npy (1.00 MB total)\n",
      "\n",
      "================================================================================\n",
      "PHASE 0 COMPLETE ‚úÖ\n",
      "Ready for Phase 1: Implement modular SSL components\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ DATA SPLITS CREATED:\")\n",
    "print(f\"   Training:   {len(train_df):5} segments (93.6%)\")\n",
    "print(f\"   Validation: {len(val_df):5} segments (4.5%)\")\n",
    "print(f\"   Test:       {len(test_df):5} segments (1.9%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ QUALITY ASSURANCE:\")\n",
    "print(f\"   Total unique subjects: {len(signal_metadata_df['subject_id'].unique())}\")\n",
    "print(f\"   Train unique subjects: {len(train_df['subject_id'].unique())}\")\n",
    "print(f\"   Val unique subjects:   {len(val_df['subject_id'].unique())}\")\n",
    "print(f\"   Test unique subjects:  {len(test_df['subject_id'].unique())}\")\n",
    "\n",
    "print(f\"\\n‚úÖ GROUND TRUTH PREPARATION:\")\n",
    "print(f\"   Wavelet denoised signals: {denoised_count}\")\n",
    "print(f\"   Index file: {index_path}\")\n",
    "print(f\"   Denoised dir: {DENOISED_SIGNALS_DIR}\")\n",
    "\n",
    "print(f\"\\n‚úÖ OUTPUT FILES:\")\n",
    "print(f\"   1. ssl_pretraining_data.parquet ({train_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   2. ssl_validation_data.parquet ({val_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   3. ssl_test_data.parquet ({test_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   4. denoised_signal_index.json ({index_path.stat().st_size / 1024:.2f} KB)\")\n",
    "print(f\"   5. denoised_signals/*.npy ({DENOISED_SIGNALS_DIR.stat().st_size / 1024**2:.2f} MB total)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETE ‚úÖ\")\n",
    "print(\"Ready for Phase 1: Implement modular SSL components\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
