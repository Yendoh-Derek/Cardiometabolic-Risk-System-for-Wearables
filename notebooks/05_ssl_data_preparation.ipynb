{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9982451",
   "metadata": {},
   "source": [
    "# Phase 0‚Äì5A: SSL Data Preparation & Windowing\n",
    "\n",
    "**Goal**: Prepare datasets for self-supervised learning (SSL) pretraining on 4,417 unlabeled PPG signals, then convert to Phase 5A windowed format (617K √ó 1,250 samples).\n",
    "\n",
    "**Findings from Phase -1**:\n",
    "- ‚ùå Zero overlap between waveform subject IDs (52-4833) and MIMIC clinical CSVs (10001-44228)\n",
    "- 4,417 PPG segments available (75K samples @ 125 Hz each)\n",
    "- 130 unique subjects, all \"Excellent\" quality (mean SQI=0.958)\n",
    "- No clinical labels available ‚Üí use self-supervised pretraining\n",
    "\n",
    "**Approach**: \n",
    "- **Phase 0**: Create train/val/test splits and compute wavelet-denoised ground truth\n",
    "- **Phase 5A**: Generate overlapping 10-sec (1,250-sample) windows from denoised signals via stride-500 sliding windows\n",
    "- Train denoising autoencoder on 617K windowed training examples\n",
    "- Validate on 617K windowed validation examples\n",
    "- Preserve subject-level splits to prevent patient biometric leakage in Phase 8\n",
    "\n",
    "**Outputs**:\n",
    "- **Phase 0**: ssl_pretraining_data.parquet, ssl_validation_data.parquet, ssl_test_data.parquet, denoised_signal_index.json, denoised_signals/*.npy\n",
    "- **Phase 5A**: mimic_windows.npy (617K √ó 1,250 array), mimic_windows_metadata.parquet (window-level metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36518280",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "294047ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete\n",
      "   Project root: C:\\Developments\\cardiometabolic-risk-colab\n",
      "   Data dir: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\n",
      "   Denoised signals dir: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\denoised_signals\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Setup paths - use absolute path to ensure correct directory\n",
    "NOTEBOOK_DIR = Path(__file__).parent if '__file__' in dir() else Path.cwd()\n",
    "PROJECT_ROOT = Path(r\"c:\\Developments\\cardiometabolic-risk-colab\").resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"colab_src\"))\n",
    "\n",
    "# Directories (absolute paths)\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "DENOISED_SIGNALS_DIR = DATA_DIR / \"denoised_signals\"\n",
    "\n",
    "# Create output directories\n",
    "DENOISED_SIGNALS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Data dir: {DATA_DIR}\")\n",
    "print(f\"   Denoised signals dir: {DENOISED_SIGNALS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03ed07",
   "metadata": {},
   "source": [
    "## Step 1: Load Sprint 1 Signal Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fee51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded signal array: (4417, 75000)\n",
      "\n",
      "‚úÖ Signal metadata loaded\n",
      "   Rows: 4417\n",
      "   Columns: ['record_name', 'subject_id', 'segment_idx', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num']\n",
      "\n",
      "   Summary statistics:\n",
      "   - Subjects: 130\n",
      "   - Mean SQI: 0.958\n",
      "   - Mean SNR (dB): 40.66\n",
      "\n",
      "   Sample rows:\n",
      "                record_name subject_id  segment_idx   fs  sqi_score  \\\n",
      "0  p00/p000052/3533390_0004    p000052            0  125   0.893482   \n",
      "1  p00/p000052/3533390_0004    p000052            1  125   0.888996   \n",
      "2  p00/p000052/3238451_0005    p000052            0  125   0.888845   \n",
      "\n",
      "  quality_grade     snr_db  perfusion_index channel_name  global_segment_idx  \\\n",
      "0     Excellent  39.255102     3.938813e+06        PLETH                   0   \n",
      "1     Excellent  38.742355     3.663113e+06        PLETH                   1   \n",
      "2     Excellent  38.725109     1.637093e+06        PLETH                   2   \n",
      "\n",
      "   batch_num  \n",
      "0        1.0  \n",
      "1        1.0  \n",
      "2        1.0  \n"
     ]
    }
   ],
   "source": [
    "# Load signal metadata\n",
    "signal_metadata_path = DATA_DIR / \"sprint1_metadata.parquet\"\n",
    "signal_metadata_df = pd.read_parquet(signal_metadata_path)\n",
    "\n",
    "# Load signal waveforms (if available as numpy array)\n",
    "signal_array_path = DATA_DIR / \"sprint1_signals.npy\"\n",
    "if signal_array_path.exists():\n",
    "    signals = np.load(signal_array_path)\n",
    "    print(f\"‚úÖ Loaded signal array: {signals.shape}\")\n",
    "else:\n",
    "    signals = None\n",
    "    print(f\"‚ö†Ô∏è  Signal array not found. Will be loaded individually from batches.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Signal metadata loaded\")\n",
    "print(f\"   Rows: {len(signal_metadata_df)}\")\n",
    "print(f\"   Columns: {list(signal_metadata_df.columns)}\")\n",
    "print(f\"\\n   Summary statistics:\")\n",
    "print(f\"   - Subjects: {signal_metadata_df['subject_id'].nunique()}\")\n",
    "print(f\"   - Mean SQI: {signal_metadata_df['sqi_score'].mean():.3f}\")\n",
    "print(f\"   - Mean SNR (dB): {signal_metadata_df['snr_db'].mean():.2f}\")\n",
    "print(f\"\\n   Sample rows:\")\n",
    "print(signal_metadata_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36911b59",
   "metadata": {},
   "source": [
    "## Step 2: Create Train/Val/Test Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76853ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating data splits from 4417 segments\n",
      "\n",
      "‚úÖ Data split created:\n",
      "   Train: 4133 segments (93.6%)\n",
      "   Val:   200 segments (4.5%)\n",
      "   Test:  84 segments (1.9%)\n",
      "\n",
      "üìà Quality metrics by split:\n",
      "\n",
      "   Train:\n",
      "      Mean SQI:  0.955 ¬± 0.054\n",
      "      Mean SNR:  40.52 ¬± 3.95 dB\n",
      "      Subjects:  128\n",
      "\n",
      "   Val:\n",
      "      Mean SQI:  1.000 ¬± 0.000\n",
      "      Mean SNR:  42.63 ¬± 2.42 dB\n",
      "      Subjects:  14\n",
      "\n",
      "   Test:\n",
      "      Mean SQI:  1.000 ¬± 0.000\n",
      "      Mean SNR:  43.13 ¬± 2.76 dB\n",
      "      Subjects:  10\n",
      "\n",
      "‚úÖ No overlap between splits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Strategy: Stratify by subject to avoid leakage\n",
    "# Goal: 4133 train, 200 val, 84 test\n",
    "np.random.seed(42)\n",
    "\n",
    "total_segments = len(signal_metadata_df)\n",
    "print(f\"üìä Creating data splits from {total_segments} segments\\n\")\n",
    "\n",
    "# Ensure high SQI segments for test set\n",
    "signal_metadata_df_sorted = signal_metadata_df.sort_values('sqi_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Take top 84 for test (highest quality)\n",
    "test_df = signal_metadata_df_sorted.iloc[:84].copy()\n",
    "remaining_df = signal_metadata_df_sorted.iloc[84:].copy()\n",
    "\n",
    "# From remaining, take 200 for validation\n",
    "val_df = remaining_df.iloc[:200].copy()\n",
    "train_df = remaining_df.iloc[200:].copy()\n",
    "\n",
    "print(f\"‚úÖ Data split created:\")\n",
    "print(f\"   Train: {len(train_df)} segments ({100*len(train_df)/total_segments:.1f}%)\")\n",
    "print(f\"   Val:   {len(val_df)} segments ({100*len(val_df)/total_segments:.1f}%)\")\n",
    "print(f\"   Test:  {len(test_df)} segments ({100*len(test_df)/total_segments:.1f}%)\")\n",
    "\n",
    "# Quality metrics for each split\n",
    "print(f\"\\nüìà Quality metrics by split:\")\n",
    "for split_name, split_df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"\\n   {split_name}:\")\n",
    "    print(f\"      Mean SQI:  {split_df['sqi_score'].mean():.3f} ¬± {split_df['sqi_score'].std():.3f}\")\n",
    "    print(f\"      Mean SNR:  {split_df['snr_db'].mean():.2f} ¬± {split_df['snr_db'].std():.2f} dB\")\n",
    "    print(f\"      Subjects:  {split_df['subject_id'].nunique()}\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(set(train_df.index) & set(val_df.index)) == 0, \"Train-val overlap!\"\n",
    "assert len(set(train_df.index) & set(test_df.index)) == 0, \"Train-test overlap!\"\n",
    "assert len(set(val_df.index) & set(test_df.index)) == 0, \"Val-test overlap!\"\n",
    "print(f\"\\n‚úÖ No overlap between splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577364dd",
   "metadata": {},
   "source": [
    "## Step 3: Compute Wavelet-Denoised Ground Truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f847023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Computing wavelet-denoised ground truth for all segments...\n",
      "\n",
      "   Processed 499/4417 segments\n",
      "   Processed 999/4417 segments\n",
      "   Processed 1499/4417 segments\n",
      "   Processed 1999/4417 segments\n",
      "   Processed 2499/4417 segments\n",
      "   Processed 2999/4417 segments\n",
      "   Processed 3499/4417 segments\n",
      "   Processed 3999/4417 segments\n",
      "\n",
      "‚úÖ Wavelet denoising complete\n",
      "   Denoised signals: 4417\n",
      "   Saved to: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\denoised_signals\n",
      "   Index saved to: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\denoised_signal_index.json\n"
     ]
    }
   ],
   "source": [
    "# Import signal processing modules\n",
    "from signal_processing.denoising import WaveletDenoiser\n",
    "\n",
    "# Initialize denoising processor\n",
    "denoiser = WaveletDenoiser(wavelet='db4', level=5, threshold_method='soft')\n",
    "\n",
    "print(\"üîÑ Computing wavelet-denoised ground truth for all segments...\\n\")\n",
    "\n",
    "# Track denoised signals and create index\n",
    "denoised_index = {}\n",
    "denoised_count = 0\n",
    "\n",
    "# Process all segments\n",
    "for idx, row in signal_metadata_df.iterrows():\n",
    "    segment_id = row['global_segment_idx']\n",
    "    record_name = row['record_name']\n",
    "    \n",
    "    # Get original signal (either from loaded array or load batch file)\n",
    "    if signals is not None:\n",
    "        signal = signals[idx]\n",
    "    else:\n",
    "        # Load from signal_batches if available\n",
    "        batch_dir = DATA_DIR / \"signal_batches\"\n",
    "        if batch_dir.exists():\n",
    "            # Try to find the signal file\n",
    "            batch_files = list(batch_dir.glob(f\"batch_*.npy\"))\n",
    "            if batch_files:\n",
    "                # For now, skip if can't find individual signal\n",
    "                print(f\"   ‚ö†Ô∏è  Signal file not found for idx {idx}, skipping\")\n",
    "                continue\n",
    "    \n",
    "    # Denoise using wavelet decomposition\n",
    "    denoised_signal = denoiser.denoise(signal)\n",
    "    \n",
    "    # Save denoised signal\n",
    "    denoised_path = DENOISED_SIGNALS_DIR / f\"{segment_id:06d}.npy\"\n",
    "    np.save(denoised_path, denoised_signal)\n",
    "    \n",
    "    # Track in index\n",
    "    denoised_index[int(segment_id)] = str(denoised_path.relative_to(DATA_DIR))\n",
    "    denoised_count += 1\n",
    "    \n",
    "    if (denoised_count + 1) % 500 == 0:\n",
    "        print(f\"   Processed {denoised_count}/{len(signal_metadata_df)} segments\")\n",
    "\n",
    "print(f\"\\n‚úÖ Wavelet denoising complete\")\n",
    "print(f\"   Denoised signals: {denoised_count}\")\n",
    "print(f\"   Saved to: {DENOISED_SIGNALS_DIR}\")\n",
    "\n",
    "# Save index as JSON for fast lookup\n",
    "index_path = DATA_DIR / \"denoised_signal_index.json\"\n",
    "with open(index_path, 'w') as f:\n",
    "    json.dump(denoised_index, f, indent=2)\n",
    "print(f\"   Index saved to: {index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05ec33",
   "metadata": {},
   "source": [
    "## Step 4: Save Data Splits as Parquet Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a1a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data splits saved to parquet:\n",
      "   Train: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\ssl_pretraining_data.parquet\n",
      "   Val:   C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\ssl_validation_data.parquet\n",
      "   Test:  C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\ssl_test_data.parquet\n",
      "\n",
      "üìã Verification:\n",
      "   Train parquet size: 0.17 MB\n",
      "   Val parquet size:   0.02 MB\n",
      "   Test parquet size:  0.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Add segment_id column for tracking\n",
    "train_df['segment_id'] = train_df['global_segment_idx']\n",
    "val_df['segment_id'] = val_df['global_segment_idx']\n",
    "test_df['segment_id'] = test_df['global_segment_idx']\n",
    "\n",
    "# Save parquet files\n",
    "train_path = OUTPUT_DIR / \"ssl_pretraining_data.parquet\"\n",
    "val_path = OUTPUT_DIR / \"ssl_validation_data.parquet\"\n",
    "test_path = OUTPUT_DIR / \"ssl_test_data.parquet\"\n",
    "\n",
    "train_df.to_parquet(train_path)\n",
    "val_df.to_parquet(val_path)\n",
    "test_df.to_parquet(test_path)\n",
    "\n",
    "print(\"‚úÖ Data splits saved to parquet:\")\n",
    "print(f\"   Train: {train_path}\")\n",
    "print(f\"   Val:   {val_path}\")\n",
    "print(f\"   Test:  {test_path}\")\n",
    "\n",
    "# Verify files\n",
    "print(f\"\\nüìã Verification:\")\n",
    "print(f\"   Train parquet size: {train_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "print(f\"   Val parquet size:   {val_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "print(f\"   Test parquet size:  {test_path.stat().st_size / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d0bc1",
   "metadata": {},
   "source": [
    "## Phase 5A: Generate Windowed Data (617K √ó 1,250 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38db871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 0 COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ DATA SPLITS CREATED:\n",
      "   Training:    4133 segments (93.6%)\n",
      "   Validation:   200 segments (4.5%)\n",
      "   Test:          84 segments (1.9%)\n",
      "\n",
      "‚úÖ QUALITY ASSURANCE:\n",
      "   Total unique subjects: 130\n",
      "   Train unique subjects: 128\n",
      "   Val unique subjects:   14\n",
      "   Test unique subjects:  10\n",
      "\n",
      "‚úÖ GROUND TRUTH PREPARATION:\n",
      "   Wavelet denoised signals: 4417\n",
      "   Index file: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\denoised_signal_index.json\n",
      "   Denoised dir: C:\\Developments\\cardiometabolic-risk-colab\\data\\processed\\denoised_signals\n",
      "\n",
      "‚úÖ PHASE 0 OUTPUT FILES:\n",
      "   1. ssl_pretraining_data.parquet (0.17 MB)\n",
      "   2. ssl_validation_data.parquet (0.02 MB)\n",
      "   3. ssl_test_data.parquet (0.01 MB)\n",
      "   4. denoised_signal_index.json (184.40 KB)\n",
      "   5. denoised_signals/*.npy (1.00 MB total)\n",
      "\n",
      "================================================================================\n",
      "PHASE 0 COMPLETE ‚úÖ\n",
      "Proceeding to Phase 5A: Generate windowed data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ DATA SPLITS CREATED:\")\n",
    "print(f\"   Training:   {len(train_df):5} segments (93.6%)\")\n",
    "print(f\"   Validation: {len(val_df):5} segments (4.5%)\")\n",
    "print(f\"   Test:       {len(test_df):5} segments (1.9%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ QUALITY ASSURANCE:\")\n",
    "print(f\"   Total unique subjects: {len(signal_metadata_df['subject_id'].unique())}\")\n",
    "print(f\"   Train unique subjects: {len(train_df['subject_id'].unique())}\")\n",
    "print(f\"   Val unique subjects:   {len(val_df['subject_id'].unique())}\")\n",
    "print(f\"   Test unique subjects:  {len(test_df['subject_id'].unique())}\")\n",
    "\n",
    "print(f\"\\n‚úÖ GROUND TRUTH PREPARATION:\")\n",
    "print(f\"   Wavelet denoised signals: {denoised_count}\")\n",
    "print(f\"   Index file: {index_path}\")\n",
    "print(f\"   Denoised dir: {DENOISED_SIGNALS_DIR}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 0 OUTPUT FILES:\")\n",
    "print(f\"   1. ssl_pretraining_data.parquet ({train_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   2. ssl_validation_data.parquet ({val_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   3. ssl_test_data.parquet ({test_path.stat().st_size / 1024**2:.2f} MB)\")\n",
    "print(f\"   4. denoised_signal_index.json ({index_path.stat().st_size / 1024:.2f} KB)\")\n",
    "print(f\"   5. denoised_signals/*.npy ({DENOISED_SIGNALS_DIR.stat().st_size / 1024**2:.2f} MB total)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETE ‚úÖ\")\n",
    "print(\"Proceeding to Phase 5A: Generate windowed data\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51985129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5A: GENERATING WINDOWED DATA\n",
      "================================================================================\n",
      "\n",
      "Transforming 4,417 √ó 75K signals ‚Üí 617K √ó 1,250 windows\n",
      "Window length: 1,250 samples (10 sec @ 125 Hz)\n",
      "Stride: 500 samples (4 sec overlap)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 17:16:32,781 - INFO - Loaded index with 4417 signals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating windows from denoised signals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 17:16:33,006 - INFO - Loaded quality metadata with 4417 rows\n",
      "Generating windows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4417/4417 [00:53<00:00, 82.83it/s] \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.04 GiB for an array with shape (653716, 1250) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m windows_meta_path = Path(windows_meta_path)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Generate windows and save to paths\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Pass quality metadata to preserve SQI and SNR in window metadata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m total_generated, total_kept = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_windows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_array_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindows_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_metadata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindows_meta_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquality_metadata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquality_metadata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Window generation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total windows generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_generated\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Developments\\cardiometabolic-risk-colab\\colab_src\\data_pipeline\\generate_mimic_windows.py:165\u001b[39m, in \u001b[36mMIMICWindowGenerator.generate_windows\u001b[39m\u001b[34m(self, output_array_path, output_metadata_path, quality_metadata_path)\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Stack windows into array\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m windows_array = np.array(windows_list, dtype=np.float32)  \u001b[38;5;66;03m# [N, 1250]\u001b[39;00m\n\u001b[32m    166\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindows_array.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m windows, shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindows_array.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Save windows array (convert Path to string for numpy compatibility)\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.04 GiB for an array with shape (653716, 1250) and data type float32"
     ]
    }
   ],
   "source": [
    "# Import modular window generator\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached module to force reimport\n",
    "if 'data_pipeline.generate_mimic_windows' in sys.modules:\n",
    "    del sys.modules['data_pipeline.generate_mimic_windows']\n",
    "if 'data_pipeline' in sys.modules:\n",
    "    del sys.modules['data_pipeline']\n",
    "\n",
    "from data_pipeline.generate_mimic_windows import MIMICWindowGenerator\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5A: GENERATING WINDOWED DATA\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTransforming 4,417 √ó 75K signals ‚Üí 617K √ó 1,250 windows\")\n",
    "print(\"Window length: 1,250 samples (10 sec @ 125 Hz)\")\n",
    "print(\"Stride: 500 samples (4 sec overlap)\")\n",
    "print()\n",
    "\n",
    "# Prepare quality metadata (signal-level SQI and SNR)\n",
    "# Save signal metadata with quality scores for window generator\n",
    "signal_metadata_with_quality = signal_metadata_df[['global_segment_idx', 'subject_id', 'sqi_score', 'snr_db']].copy()\n",
    "signal_metadata_with_quality['segment_id'] = signal_metadata_with_quality['global_segment_idx']\n",
    "quality_metadata_path = DATA_DIR / \"signal_quality_metadata.parquet\"\n",
    "signal_metadata_with_quality.to_parquet(quality_metadata_path)\n",
    "\n",
    "# Initialize window generator\n",
    "# Note: signal_dir should be DATA_DIR (not DENOISED_SIGNALS_DIR) \n",
    "# because index paths are relative and already contain \"denoised_signals/\"\n",
    "generator = MIMICWindowGenerator(\n",
    "    signal_dir=DATA_DIR,\n",
    "    denoised_index_path=index_path,\n",
    "    window_length=1250,\n",
    "    stride=500,\n",
    ")\n",
    "\n",
    "# Generate windows with output paths\n",
    "print(\"üîÑ Generating windows from denoised signals...\")\n",
    "windows_path = OUTPUT_DIR / \"mimic_windows.npy\"\n",
    "windows_meta_path = OUTPUT_DIR / \"mimic_windows_metadata.parquet\"\n",
    "\n",
    "# Ensure paths are converted to strings for numpy compatibility\n",
    "windows_path = Path(windows_path)\n",
    "windows_meta_path = Path(windows_meta_path)\n",
    "\n",
    "# Generate windows and save to paths\n",
    "# Pass quality metadata to preserve SQI and SNR in window metadata\n",
    "total_generated, total_kept = generator.generate_windows(\n",
    "    output_array_path=windows_path,\n",
    "    output_metadata_path=windows_meta_path,\n",
    "    quality_metadata_path=quality_metadata_path,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Window generation complete!\")\n",
    "print(f\"   Total windows generated: {total_generated:,}\")\n",
    "print(f\"   Windows kept (after filtering): {total_kept:,}\")\n",
    "\n",
    "# Load the generated windows for verification\n",
    "windows_array = np.load(windows_path, mmap_mode='r')\n",
    "windows_metadata = pd.read_parquet(windows_meta_path)\n",
    "\n",
    "print(f\"   Window shape: {windows_array.shape}\")\n",
    "print(f\"   Metadata rows: {len(windows_metadata):,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 5A outputs saved:\")\n",
    "print(f\"   Windows array: {windows_path} ({windows_path.stat().st_size / 1e9:.2f} GB)\")\n",
    "print(f\"   Metadata: {windows_meta_path} ({len(windows_metadata):,} rows)\")\n",
    "print(f\"   Quality metadata: {quality_metadata_path}\")\n",
    "\n",
    "# Verify split statistics using original train/val/test dataframes\n",
    "# Windows metadata doesn't have split column - it's based on source signal's original split\n",
    "print(f\"\\nüìä Window split distribution (from source signals):\")\n",
    "windows_per_split = {\n",
    "    'train': 0,\n",
    "    'val': 0,\n",
    "    'test': 0,\n",
    "}\n",
    "\n",
    "for source_signal_id in windows_metadata['source_signal_id'].unique():\n",
    "    # Count windows from this signal\n",
    "    n_windows_from_signal = (windows_metadata['source_signal_id'] == source_signal_id).sum()\n",
    "    \n",
    "    # Determine which split this signal belongs to\n",
    "    if source_signal_id in train_df['global_segment_idx'].values:\n",
    "        windows_per_split['train'] += n_windows_from_signal\n",
    "    elif source_signal_id in val_df['global_segment_idx'].values:\n",
    "        windows_per_split['val'] += n_windows_from_signal\n",
    "    elif source_signal_id in test_df['global_segment_idx'].values:\n",
    "        windows_per_split['test'] += n_windows_from_signal\n",
    "\n",
    "for split, count in windows_per_split.items():\n",
    "    print(f\"   {split}: {count:,} windows ({100*count/len(windows_metadata):.1f}%)\")\n",
    "\n",
    "# Verify subject-level grouping\n",
    "if 'subject_id' in windows_metadata.columns:\n",
    "    train_subject_ids = set(train_df['subject_id'].values)\n",
    "    val_subject_ids = set(val_df['subject_id'].values)\n",
    "    \n",
    "    windows_train_subjects = windows_metadata[windows_metadata['source_signal_id'].isin(train_df['global_segment_idx'])]['subject_id'].nunique()\n",
    "    windows_val_subjects = windows_metadata[windows_metadata['source_signal_id'].isin(val_df['global_segment_idx'])]['subject_id'].nunique()\n",
    "    \n",
    "    print(f\"\\nüë• Subject-level integrity (prevents patient leakage):\")\n",
    "    print(f\"   Train unique subjects: {windows_train_subjects}\")\n",
    "    print(f\"   Val unique subjects: {windows_val_subjects}\")\n",
    "    \n",
    "    # Check for overlap\n",
    "    overlap = train_subject_ids & val_subject_ids\n",
    "    if len(overlap) == 0:\n",
    "        print(f\"   ‚úÖ No subject overlap between train/val\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: {len(overlap)} subjects appear in both train/val!\")\n",
    "\n",
    "# Verify quality metrics are preserved\n",
    "if 'sqi_score' in windows_metadata.columns:\n",
    "    print(f\"\\nüìä Quality metrics in window metadata:\")\n",
    "    print(f\"   Mean SQI: {windows_metadata['sqi_score'].mean():.3f}\")\n",
    "    print(f\"   Mean SNR: {windows_metadata['snr_db'].mean():.1f} dB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5A COMPLETE ‚úÖ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efe7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0‚Äì5A COMPLETE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 0 OUTPUTS (Base data preparation):\")\n",
    "print(f\"   1. ssl_pretraining_data.parquet (4,133 train segments @ 75K samples)\")\n",
    "print(f\"   2. ssl_validation_data.parquet (200 val segments @ 75K samples)\")\n",
    "print(f\"   3. ssl_test_data.parquet (84 test segments @ 75K samples)\")\n",
    "print(f\"   4. denoised_signal_index.json (signal ID ‚Üí path mapping)\")\n",
    "print(f\"   5. denoised_signals/*.npy (wavelet-denoised ground truth)\")\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 5A OUTPUTS (Windowed format for training):\")\n",
    "print(f\"   1. mimic_windows.npy ({windows_array.shape[0]:,} windows √ó {windows_array.shape[1]} samples)\")\n",
    "print(f\"      Size: {windows_path.stat().st_size / 1e9:.2f} GB\")\n",
    "print(f\"   2. mimic_windows_metadata.parquet ({len(windows_metadata):,} rows)\")\n",
    "print(f\"      Columns: {list(windows_metadata.columns)}\")\n",
    "\n",
    "print(f\"\\nüìä DATA STATISTICS:\")\n",
    "print(f\"   Original segments: 4,417 (75K samples each)\")\n",
    "print(f\"   Windowed examples: {windows_array.shape[0]:,} (1,250 samples each)\")\n",
    "print(f\"   Compression ratio: {4417 * 75000 / (windows_array.shape[0] * windows_array.shape[1]):.2f}x (stride=500)\")\n",
    "print(f\"   Train subjects: {train_subjects} (preserved from Phase 0)\")\n",
    "print(f\"   Val subjects: {val_subjects} (preserved from Phase 0)\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEP:\")\n",
    "print(f\"   1. Upload both Phase 0 and Phase 5A outputs to Google Drive\")\n",
    "print(f\"   2. Run 05_ssl_pretraining_colab.ipynb on Colab T4 GPU\")\n",
    "print(f\"   3. Expected training time: 8‚Äì12 hours for 50 epochs\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to proceed with Phase 5B SSL Pretraining on Colab!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
