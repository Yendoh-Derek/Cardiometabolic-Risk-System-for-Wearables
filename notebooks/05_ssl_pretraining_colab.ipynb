{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardiometabolic Risk: SSL Pretraining on Colab\n",
    "\n",
    "**Phase 5**: Train a self-supervised PPG encoder on 4,133 signals using Colab T4 GPU\n",
    "\n",
    "**Expected runtime**: 8\u201312 hours (50 epochs)  \n",
    "**Output**: Pretrained encoder checkpoint + training metrics\n",
    "\n",
    "**Prerequisites**:\n",
    "- Data uploaded to Google Drive: `/MyDrive/cardiometabolic-risk-colab/data/processed/`\n",
    "- GitHub repo exists and is public\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Mount Drive & Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "COLAB_DRIVE_PATH = Path('/content/drive/MyDrive/cardiometabolic-risk-colab')\n",
    "print(f\"\u2705 Drive mounted: {COLAB_DRIVE_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "repo_dir = Path('/content/repo')\n",
    "repo_url = \"https://github.com/Yendoh-Derek/Cardiometabolic-Risk-System-for-Wearables.git\"\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", repo_url, str(repo_dir)], check=True)\n",
    "    print(f\"\u2705 Repo cloned: {repo_dir}\")\n",
    "else:\n",
    "    print(f\"\u2705 Repo already present: {repo_dir}\")\n",
    "\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2705 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f\"\\n\u2705 GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(repo_dir / \"colab_src\"))\n",
    "\n",
    "from colab_src.models.ssl.config import SSLConfig\n",
    "from colab_src.models.ssl.encoder import ResNetEncoder\n",
    "from colab_src.models.ssl.decoder import ResNetDecoder\n",
    "from colab_src.models.ssl.losses import SSLLoss\n",
    "\n",
    "print(\"\u2705 All imports successful\")\n",
    "\n",
    "# Load config\n",
    "cfg = SSLConfig.from_yaml(\"configs/ssl_pretraining.yaml\")\n",
    "print(f\"\u2705 Config loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = repo_dir / \"data\" / \"processed\"\n",
    "\n",
    "# If in Colab, symlink Drive data to repo structure\n",
    "try:\n",
    "    drive_data = COLAB_DRIVE_PATH / \"data\" / \"processed\"\n",
    "    if drive_data.exists() and not data_dir.exists():\n",
    "        print(f\"Linking Drive data: {drive_data} \u2192 {data_dir}\")\n",
    "        subprocess.run([\"ln\", \"-s\", str(drive_data), str(data_dir)], check=True)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "\n",
    "# Verify required files\n",
    "required_files = {\n",
    "    \"ssl_pretraining_data.parquet\": \"Training metadata\",\n",
    "    \"ssl_validation_data.parquet\": \"Validation metadata\",\n",
    "    \"denoised_signals\": \"Ground truth signals (denoised)\",\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd0d Checking data integrity...\\n\")\n",
    "all_present = True\n",
    "\n",
    "for fname, description in required_files.items():\n",
    "    fpath = data_dir / fname\n",
    "    if fpath.exists():\n",
    "        if fpath.is_dir():\n",
    "            count = len(list(fpath.glob(\"*.npy\")))\n",
    "            print(f\"\u2705 {fname:40s} ({count:5d} files) \u2014 {description}\")\n",
    "        else:\n",
    "            size_mb = fpath.stat().st_size / 1e6\n",
    "            print(f\"\u2705 {fname:40s} ({size_mb:6.1f} MB) \u2014 {description}\")\n",
    "    else:\n",
    "        print(f\"\u274c {fname:40s} NOT FOUND \u2014 {description}\")\n",
    "        all_present = False\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n\u26a0\ufe0f  MISSING DATA FILES\")\n",
    "    print(\"\\nTo fix:\")\n",
    "    print(\"  1. Upload data from your local PC to Google Drive\")\n",
    "    print(f\"  2. Path: /MyDrive/cardiometabolic-risk-colab/data/processed/\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "    raise FileNotFoundError(\"Data files not found\")\n",
    "\n",
    "# Verify metadata\n",
    "train_meta = pd.read_parquet(data_dir / \"ssl_pretraining_data.parquet\")\n",
    "print(f\"\\n\u2705 Training dataset: {len(train_meta)} samples\")\n",
    "\n",
    "val_meta = pd.read_parquet(data_dir / \"ssl_validation_data.parquet\")\n",
    "print(f\"\u2705 Validation dataset: {len(val_meta)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 ALL DATA READY FOR TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Run Full Training (50 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for checkpoints\n",
    "checkpoint_dir = COLAB_DRIVE_PATH / \"phase5_checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\ud83d\udcc1 Checkpoints will be saved to:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\n\u23f1\ufe0f  Estimated duration: 8\u201312 hours\")\n",
    "print(f\"\ud83d\udcbe Batch size: 8 (with 4\u00d7 accumulation = eff. 32)\")\n",
    "print(f\"\ud83d\udd22 Epochs: 50\")\n",
    "print(f\"\ud83d\udcca Training samples: 4,133\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"colab_src/models/ssl/train.py\",\n",
    "    \"--config\", \"configs/ssl_pretraining.yaml\",\n",
    "    \"--device\", \"cuda\",\n",
    "    \"--num_epochs\", \"50\",\n",
    "    \"--output_dir\", str(checkpoint_dir),\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd)\n",
    "print(f\"\\n\u2705 Training complete (exit code: {result.returncode})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate & Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_file = checkpoint_dir / \"training_metrics.json\"\n",
    "\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    axes[0].plot(metrics['train_loss'], linewidth=2, color='steelblue')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Loss (MSE+SSIM+FFT)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    if 'val_loss' in metrics:\n",
    "        axes[1].plot(metrics['val_loss'], linewidth=2, color='coral')\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'loss_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\u2705 Loss curves plotted and saved\")\n",
    "    print(f\"\\n\ud83d\udcca Final metrics:\")\n",
    "    print(f\"   Train loss: {metrics['train_loss'][-1]:.4f}\")\n",
    "    if 'val_loss' in metrics:\n",
    "        print(f\"   Val loss:   {metrics['val_loss'][-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Metrics file not found: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Phase 5 Complete\n",
    "\n",
    "Checkpoints are saved to Google Drive at:\n",
    "```\n",
    "/MyDrive/cardiometabolic-risk-colab/phase5_checkpoints/\n",
    "```\n",
    "\n",
    "**Next Steps**:\n",
    "1. Phase 6: Linear probe evaluation\n",
    "2. Phase 7: Extract embeddings\n",
    "3. Phase 8: Train XGBoost models\n",
    "\n",
    "See [README.md](../README.md) for detailed instructions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "05_ssl_pretraining_colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}