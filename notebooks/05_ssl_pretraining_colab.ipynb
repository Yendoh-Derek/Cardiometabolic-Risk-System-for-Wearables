{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardiometabolic Risk: SSL Pretraining on Colab\n",
    "\n",
    "**Phase 5**: Train a self-supervised PPG encoder on 4,133 signals using Colab T4 GPU\n",
    "\n",
    "**Expected runtime**: 8‚Äì12 hours (50 epochs)  \n",
    "**Output**: Pretrained encoder checkpoint + training metrics\n",
    "\n",
    "**Prerequisites**:\n",
    "- Data uploaded to Google Drive: `/MyDrive/cardiometabolic-risk-colab/data/processed/`\n",
    "- GitHub repo exists and is public\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Mount Drive & Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "COLAB_DRIVE_PATH = Path('/content/drive/MyDrive/cardiometabolic-risk-colab')\n",
    "print(f\"‚úÖ Drive mounted: {COLAB_DRIVE_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "repo_dir = Path('/content/repo')\n",
    "repo_url = \"https://github.com/Yendoh-Derek/Cardiometabolic-Risk-System-for-Wearables.git\"\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", repo_url, str(repo_dir)], check=True)\n",
    "    print(f\"‚úÖ Repo cloned: {repo_dir}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repo already present: {repo_dir}\")\n",
    "\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úÖ GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(repo_dir / \"colab_src\"))\n",
    "\n",
    "from colab_src.models.ssl.config import SSLConfig\n",
    "from colab_src.models.ssl.encoder import ResNetEncoder\n",
    "from colab_src.models.ssl.decoder import ResNetDecoder\n",
    "from colab_src.models.ssl.losses import SSLLoss\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "\n",
    "# Load config\n",
    "cfg = SSLConfig.from_yaml(\"configs/ssl_pretraining.yaml\")\n",
    "print(f\"‚úÖ Config loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = repo_dir / \"data\" / \"processed\"\n",
    "\n",
    "# If in Colab, symlink Drive data to repo structure\n",
    "try:\n",
    "    drive_data = COLAB_DRIVE_PATH / \"data\" / \"processed\"\n",
    "    if drive_data.exists() and not data_dir.exists():\n",
    "        print(f\"Linking Drive data: {drive_data} ‚Üí {data_dir}\")\n",
    "        subprocess.run([\"ln\", \"-s\", str(drive_data), str(data_dir)], check=True)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "\n",
    "# Verify required files\n",
    "required_files = {\n",
    "    \"ssl_pretraining_data.parquet\": \"Training metadata\",\n",
    "    \"ssl_validation_data.parquet\": \"Validation metadata\",\n",
    "    \"denoised_signals\": \"Ground truth signals (denoised)\",\n",
    "}\n",
    "\n",
    "print(\"üîç Checking data integrity...\\n\")\n",
    "all_present = True\n",
    "\n",
    "for fname, description in required_files.items():\n",
    "    fpath = data_dir / fname\n",
    "    if fpath.exists():\n",
    "        if fpath.is_dir():\n",
    "            count = len(list(fpath.glob(\"*.npy\")))\n",
    "            print(f\"‚úÖ {fname:40s} ({count:5d} files) ‚Äî {description}\")\n",
    "        else:\n",
    "            size_mb = fpath.stat().st_size / 1e6\n",
    "            print(f\"‚úÖ {fname:40s} ({size_mb:6.1f} MB) ‚Äî {description}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {fname:40s} NOT FOUND ‚Äî {description}\")\n",
    "        all_present = False\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n‚ö†Ô∏è  MISSING DATA FILES\")\n",
    "    print(\"\\nTo fix:\")\n",
    "    print(\"  1. Upload data from your local PC to Google Drive\")\n",
    "    print(f\"  2. Path: /MyDrive/cardiometabolic-risk-colab/data/processed/\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "    raise FileNotFoundError(\"Data files not found\")\n",
    "\n",
    "# Verify metadata\n",
    "train_meta = pd.read_parquet(data_dir / \"ssl_pretraining_data.parquet\")\n",
    "print(f\"\\n‚úÖ Training dataset: {len(train_meta)} samples\")\n",
    "\n",
    "val_meta = pd.read_parquet(data_dir / \"ssl_validation_data.parquet\")\n",
    "print(f\"‚úÖ Validation dataset: {len(val_meta)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL DATA READY FOR TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Run Full Training (50 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for checkpoints\n",
    "checkpoint_dir = COLAB_DRIVE_PATH / \"phase5_checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Checkpoints will be saved to:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\n‚è±Ô∏è  Estimated duration: 8‚Äì12 hours\")\n",
    "print(f\"üíæ Batch size: 8 (with 4√ó accumulation = eff. 32)\")\n",
    "print(f\"üî¢ Epochs: 50\")\n",
    "print(f\"üìä Training samples: 4,133\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"colab_src.models.ssl.train\",\n",
    "    \"--config\", str(repo_dir / \"configs/ssl_pretraining.yaml\"),\n",
    "    \"--data-dir\", str(data_dir),\n",
    "    \"--device\", \"cuda\",\n",
    "    \"--epochs\", \"50\",\n",
    "]\n",
    "\n",
    "print(f\"üöÄ Starting Phase 5 training...\\n\")\n",
    "print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = subprocess.run(cmd, cwd=str(repo_dir))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ Training completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Training failed with exit code: {result.returncode}\")\n",
    "    print(\"See output above for error details\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate & Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_file = checkpoint_dir / \"training_metrics.json\"\n",
    "\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    axes[0].plot(metrics['train_loss'], linewidth=2, color='steelblue')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Loss (MSE+SSIM+FFT)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    if 'val_loss' in metrics:\n",
    "        axes[1].plot(metrics['val_loss'], linewidth=2, color='coral')\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Loss', fontsize=12)\n",
    "        axes[1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(checkpoint_dir / 'loss_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Loss curves plotted and saved\")\n",
    "    print(f\"\\nüìä Final metrics:\")\n",
    "    print(f\"   Train loss: {metrics['train_loss'][-1]:.4f}\")\n",
    "    if 'val_loss' in metrics:\n",
    "        print(f\"   Val loss:   {metrics['val_loss'][-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Metrics file not found: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 5 Complete\n",
    "\n",
    "Checkpoints are saved to Google Drive at:\n",
    "```\n",
    "/MyDrive/cardiometabolic-risk-colab/phase5_checkpoints/\n",
    "```\n",
    "\n",
    "**Next Steps**:\n",
    "1. Phase 6: Linear probe evaluation\n",
    "2. Phase 7: Extract embeddings\n",
    "3. Phase 8: Train XGBoost models\n",
    "\n",
    "See [README.md](../README.md) for detailed instructions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "05_ssl_pretraining_colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
