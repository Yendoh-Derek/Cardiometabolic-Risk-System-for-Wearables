{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002c6106",
   "metadata": {},
   "source": [
    "# Clinical Data Integration\n",
    "\n",
    "**Goal**: Merge MIMIC-III clinical tables with waveform signal segments to create a cohesive ground truth dataset.\n",
    "\n",
    "**Inputs**:\n",
    "- MIMIC-III Clinical Database (PATIENTS.csv, ADMISSIONS.csv, DIAGNOSES_ICD.csv)\n",
    "- Signal metadata from Sprint 1 (signal_clinical_integrated_dataset.parquet)\n",
    "\n",
    "**Outputs**:\n",
    "- `signal_clinical_integrated_dataset.parquet`: Single source of truth with all segment-level metadata\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load MIMIC clinical tables (with caching)\n",
    "2. Extract cardiometabolic labels (Diabetes, Hypertension, Obesity)\n",
    "3. Link waveform segments to clinical admissions (Option B)\n",
    "4. Extract and process demographics (age, sex, BMI)\n",
    "5. Compute Charlson Comorbidity Index\n",
    "6. Assemble final integrated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6a92e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3161c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Developments\\cardiometabolic-risk-colab\n",
      "colab_src exists: True\n",
      "data_pipeline exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# 1. Set project root\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"colab_src\"))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"colab_src exists:\", (PROJECT_ROOT / \"colab_src\").exists())\n",
    "print(\"data_pipeline exists:\", (PROJECT_ROOT / \"colab_src\" / \"data_pipeline\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7814998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n",
      "\n",
      "ðŸ“ Configuration:\n",
      "   Project Root:       c:\\Developments\\cardiometabolic-risk-colab\n",
      "   MIMIC Clinical Dir: data\\raw\n",
      "   Cache Dir:          data\\cache\n",
      "   Output Dir:         data\\processed\n",
      "\n",
      "âœ… MIMIC directory found with 6 CSV files:\n",
      "   - ADMISSIONS.csv\n",
      "   - CHARTEVENTS.csv\n",
      "   - DIAGNOSES_ICD.csv\n",
      "   - ICUSTAYS.csv\n",
      "   - LABEVENTS.csv\n",
      "   - PATIENTS.csv\n"
     ]
    }
   ],
   "source": [
    "# Import  modules from colab_src.data_pipeline\n",
    "from data_pipeline import (\n",
    "    MIMICClinicalExtractor, \n",
    "    ClinicalLabelExtractor, \n",
    "    WaveformClinicalLinker, \n",
    "    DemographicProcessor, \n",
    "    DatasetAssembler\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "MIMIC_CLINICAL_DIR = Path('data/raw')  # MIMIC CSV files location\n",
    "CACHE_DIR = Path('data/cache')\n",
    "OUTPUT_DIR = Path('data/processed')\n",
    "\n",
    "# Ensure output directories exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… All imports successful\")\n",
    "print(f\"\\nðŸ“ Configuration:\")\n",
    "print(f\"   Project Root:       {PROJECT_ROOT}\")\n",
    "print(f\"   MIMIC Clinical Dir: {MIMIC_CLINICAL_DIR}\")\n",
    "print(f\"   Cache Dir:          {CACHE_DIR}\")\n",
    "print(f\"   Output Dir:         {OUTPUT_DIR}\")\n",
    "\n",
    "# Check for MIMIC files\n",
    "if MIMIC_CLINICAL_DIR.exists():\n",
    "    csv_files = list(MIMIC_CLINICAL_DIR.glob('*.csv'))\n",
    "    print(f\"\\nâœ… MIMIC directory found with {len(csv_files)} CSV files:\")\n",
    "    for f in sorted(csv_files):\n",
    "        print(f\"   - {f.name}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Warning: MIMIC directory not found at {MIMIC_CLINICAL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b3819",
   "metadata": {},
   "source": [
    "## Step 1: Extract MIMIC Clinical Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a084baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Inspecting MIMIC CSV file structures:\n",
      "\n",
      "âœ… PATIENTS.csv found\n",
      "   Columns: ['row_id', 'subject_id', 'gender', 'dob', 'dod', 'dod_hosp', 'dod_ssn', 'expire_flag']\n",
      "\n",
      "âœ… ADMISSIONS.csv found\n",
      "   Columns: ['row_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'religion', 'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis', 'hospital_expire_flag', 'has_chartevents_data']\n",
      "\n",
      "âœ… DIAGNOSES_ICD.csv found\n",
      "   Columns: ['row_id', 'subject_id', 'hadm_id', 'seq_num', 'icd9_code']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect MIMIC CSV files to see actual column names\n",
    "print(\"ðŸ“‹ Inspecting MIMIC CSV file structures:\\n\")\n",
    "\n",
    "# Check PATIENTS.csv\n",
    "patients_file = MIMIC_CLINICAL_DIR / 'PATIENTS.csv'\n",
    "if patients_file.exists():\n",
    "    print(f\"âœ… PATIENTS.csv found\")\n",
    "    df_sample = pd.read_csv(patients_file, nrows=0)  # Read just header\n",
    "    print(f\"   Columns: {list(df_sample.columns)}\\n\")\n",
    "else:\n",
    "    print(f\"âš ï¸  PATIENTS.csv not found\\n\")\n",
    "\n",
    "# Check ADMISSIONS.csv\n",
    "admissions_file = MIMIC_CLINICAL_DIR / 'ADMISSIONS.csv'\n",
    "if admissions_file.exists():\n",
    "    print(f\"âœ… ADMISSIONS.csv found\")\n",
    "    df_sample = pd.read_csv(admissions_file, nrows=0)\n",
    "    print(f\"   Columns: {list(df_sample.columns)}\\n\")\n",
    "else:\n",
    "    print(f\"âš ï¸  ADMISSIONS.csv not found\\n\")\n",
    "\n",
    "# Check DIAGNOSES_ICD.csv\n",
    "diagnoses_file = MIMIC_CLINICAL_DIR / 'DIAGNOSES_ICD.csv'\n",
    "if diagnoses_file.exists():\n",
    "    print(f\"âœ… DIAGNOSES_ICD.csv found\")\n",
    "    df_sample = pd.read_csv(diagnoses_file, nrows=0)\n",
    "    print(f\"   Columns: {list(df_sample.columns)}\\n\")\n",
    "else:\n",
    "    print(f\"âš ï¸  DIAGNOSES_ICD.csv not found\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbabb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 19:05:13,968 - data_pipeline.mimic_clinical_extractor - INFO - MIMICClinicalExtractor initialized\n",
      "2026-01-11 19:05:13,970 - data_pipeline.mimic_clinical_extractor - INFO -   Clinical dir: data\\raw\n",
      "2026-01-11 19:05:13,971 - data_pipeline.mimic_clinical_extractor - INFO -   Cache dir: data\\cache\n",
      "2026-01-11 19:05:13,972 - data_pipeline.mimic_clinical_extractor - INFO - ðŸ“¦ Loading from cache: data\\cache\\mimic_clinical_tables.pkl\n",
      "2026-01-11 19:05:14,003 - data_pipeline.mimic_clinical_extractor - INFO - âœ… Loaded from cache:\n",
      "2026-01-11 19:05:14,006 - data_pipeline.mimic_clinical_extractor - INFO -    PATIENTS: 100 records\n",
      "2026-01-11 19:05:14,007 - data_pipeline.mimic_clinical_extractor - INFO -    ADMISSIONS: 129 records\n",
      "2026-01-11 19:05:14,009 - data_pipeline.mimic_clinical_extractor - INFO -    DIAGNOSES_ICD: 1761 records\n",
      "2026-01-11 19:05:14,010 - data_pipeline.mimic_clinical_extractor - INFO - \n",
      "ðŸ” Validating clinical tables...\n",
      "2026-01-11 19:05:14,017 - data_pipeline.mimic_clinical_extractor - INFO - âœ… All validations passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Clinical tables loaded and validated\n",
      "   Summary: {'n_patients': 100, 'n_admissions': 129, 'n_diagnosis_records': 1761, 'n_unique_patients_in_admissions': 100, 'n_unique_diagnoses': 581, 'date_range_admissions': (Timestamp('2102-08-29 07:15:00'), Timestamp('2202-10-03 01:45:00'))}\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC clinical tables with caching\n",
    "extractor = MIMICClinicalExtractor(MIMIC_CLINICAL_DIR, cache_dir=CACHE_DIR)\n",
    "patients_df, admissions_df, diagnoses_df = extractor.load_clinical_tables(use_cache=True)\n",
    "\n",
    "# Validate\n",
    "is_valid = extractor.validate_clinical_tables()\n",
    "\n",
    "if is_valid:\n",
    "    stats = extractor.get_summary_statistics()\n",
    "    print(f\"\\nâœ… Clinical tables loaded and validated\")\n",
    "    print(f\"   Summary: {stats}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Some validation issues found. Review warnings above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a577cfc",
   "metadata": {},
   "source": [
    "## Step 2: Extract Cardiometabolic Labels and Charlson Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189cc60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 19:05:29,935 - data_pipeline.clinical_label_extractor - INFO - ðŸ¥ Extracting cardiometabolic labels from ICD-9 codes...\n",
      "2026-01-11 19:05:29,967 - data_pipeline.clinical_label_extractor - INFO - âœ… Extracted labels for 129 admissions\n",
      "2026-01-11 19:05:29,979 - data_pipeline.clinical_label_extractor - INFO -    Diabetes:         48 ( 37.2%)\n",
      "2026-01-11 19:05:29,982 - data_pipeline.clinical_label_extractor - INFO -    Hypertension:     73 ( 56.6%)\n",
      "2026-01-11 19:05:29,990 - data_pipeline.clinical_label_extractor - INFO -    Obesity:           4 (  3.1%)\n",
      "2026-01-11 19:05:29,998 - data_pipeline.clinical_label_extractor - INFO -    Multi-morbid:     40 ( 31.0%)\n",
      "2026-01-11 19:05:30,000 - data_pipeline.clinical_label_extractor - INFO -    Healthy (0,0,0):    46 ( 35.7%)\n",
      "2026-01-11 19:05:30,001 - data_pipeline.clinical_label_extractor - INFO - ðŸ“Š Computing Charlson Comorbidity Index...\n",
      "2026-01-11 19:05:30,103 - data_pipeline.clinical_label_extractor - INFO - âœ… Computed CCI for 129 admissions\n",
      "2026-01-11 19:05:30,105 - data_pipeline.clinical_label_extractor - INFO -    Mean CCI:     1.57\n",
      "2026-01-11 19:05:30,107 - data_pipeline.clinical_label_extractor - INFO -    Median CCI:      1\n",
      "2026-01-11 19:05:30,109 - data_pipeline.clinical_label_extractor - INFO -    Range:      [0, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Labels and Charlson Index extracted\n",
      "   Labels shape: (129, 4)\n",
      "   CCI shape:    (129, 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels\n",
    "label_extractor = ClinicalLabelExtractor()\n",
    "labels_df = label_extractor.extract_labels_per_admission(diagnoses_df)\n",
    "\n",
    "# Compute Charlson Index\n",
    "cci_df = label_extractor.compute_charlson_index(diagnoses_df)\n",
    "\n",
    "print(f\"\\nâœ… Labels and Charlson Index extracted\")\n",
    "print(f\"   Labels shape: {labels_df.shape}\")\n",
    "print(f\"   CCI shape:    {cci_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bf763",
   "metadata": {},
   "source": [
    "## Step 3: Load Signal Metadata from Sprint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f355b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded signal metadata from Sprint 1\n",
      "   Path:  data\\processed\\sprint1_metadata.parquet\n",
      "   Shape: (4417, 11)\n",
      "   Columns: ['record_name', 'subject_id', 'segment_idx', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num']\n",
      "\n",
      "   Sample (first 3 rows):\n",
      "                record_name subject_id  segment_idx   fs  sqi_score  \\\n",
      "0  p00/p000052/3533390_0004    p000052            0  125   0.893482   \n",
      "1  p00/p000052/3533390_0004    p000052            1  125   0.888996   \n",
      "2  p00/p000052/3238451_0005    p000052            0  125   0.888845   \n",
      "\n",
      "  quality_grade     snr_db  perfusion_index channel_name  global_segment_idx  \\\n",
      "0     Excellent  39.255102     3.938813e+06        PLETH                   0   \n",
      "1     Excellent  38.742355     3.663113e+06        PLETH                   1   \n",
      "2     Excellent  38.725109     1.637093e+06        PLETH                   2   \n",
      "\n",
      "   batch_num  \n",
      "0        1.0  \n",
      "1        1.0  \n",
      "2        1.0  \n"
     ]
    }
   ],
   "source": [
    "# Load signal metadata from Sprint 1\n",
    "signal_metadata_path = OUTPUT_DIR / 'sprint1_metadata.parquet'\n",
    "signal_metadata_df = pd.read_parquet(signal_metadata_path)\n",
    "\n",
    "print(f\"âœ… Loaded signal metadata from Sprint 1\")\n",
    "print(f\"   Path:  {signal_metadata_path}\")\n",
    "print(f\"   Shape: {signal_metadata_df.shape}\")\n",
    "print(f\"   Columns: {list(signal_metadata_df.columns)}\")\n",
    "print(f\"\\n   Sample (first 3 rows):\")\n",
    "print(signal_metadata_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802ada86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3b: EXTRACT SUBJECT IDs FROM SIGNAL RECORDS & FIND INTERSECTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Extracting subject IDs from signal metadata...\n",
      "\n",
      "âœ… Extracted subject IDs from all 4417 records\n",
      "   Sample records:\n",
      "                record_name subject_id  demo_id  real_subject_id  \\\n",
      "0  p00/p000052/3533390_0004    p000052       52            10052   \n",
      "1  p00/p000052/3533390_0004    p000052       52            10052   \n",
      "2  p00/p000052/3238451_0005    p000052       52            10052   \n",
      "3  p00/p000052/3238451_0005    p000052       52            10052   \n",
      "4  p00/p000052/3238451_0005    p000052       52            10052   \n",
      "\n",
      "   subject_id_int  \n",
      "0              52  \n",
      "1              52  \n",
      "2              52  \n",
      "3              52  \n",
      "4              52  \n",
      "\n",
      "================================================================================\n",
      "INTERSECTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Dataset Overview:\n",
      "   Clinical subjects (MIMIC CSV): 100\n",
      "   Clinical subject ID range: 10006 - 44228\n",
      "   Signal metadata records: 4417\n",
      "   Unique demo_ids in signals: 130\n",
      "   Unique real_subject_ids in signals: 130\n",
      "   Signal demo_ids range: 52 - 4833\n",
      "\n",
      "âœ… INTERSECTION FOUND:\n",
      "   Segments in intersection: 282\n",
      "   Subjects in intersection: 1\n",
      "   Coverage: 6.4% of all signals\n",
      "   Matching method: real_subject_id (10000 + demo_id)\n",
      "\n",
      "   Segments per subject (statistics):\n",
      "      Mean: 282.0\n",
      "      Median: 282\n",
      "      Min: 282\n",
      "      Max: 282\n",
      "\n",
      "   All 1 subjects:\n",
      "      10124:  282 segments\n",
      "\n",
      "âš ï¸  Subjects WITHOUT records: 99 out of 100\n",
      "   [10006, 10011, 10013, 10017, 10019, 10026, 10027, 10029, 10032, 10033, 10035, 10036, 10038, 10040, 10042, 10043, 10044, 10045, 10046, 10056]... and 79 more\n",
      "\n",
      "âœ… Saved 282 intersection segments\n",
      "   Location: data\\processed\\intersection_segments.csv\n",
      "\n",
      "âœ… Ready to filter and link with clinical data\n"
     ]
    }
   ],
   "source": [
    "# Investigate the signal metadata source - extract real MIMIC IDs from record paths\n",
    "print(\"ðŸ” INVESTIGATING SIGNAL METADATA - EXTRACTING REAL MIMIC IDs:\\n\")\n",
    "\n",
    "print(f\"Signal metadata columns: {list(signal_metadata_df.columns)}\")\n",
    "print(f\"\\nSample record_name values (first 5):\")\n",
    "for i, record in enumerate(signal_metadata_df['record_name'].head(5)):\n",
    "    print(f\"   {i+1}. {record}\")\n",
    "\n",
    "print(f\"\\n--- UNDERSTANDING RECORD PATH STRUCTURE ---\")\n",
    "print(f\"Format appears to be: p00/p######/########_####\")\n",
    "print(f\"  - p00: folder prefix\")\n",
    "print(f\"  - p######: extracted subject ID (e.g., p000052 â†’ 52)\")\n",
    "print(f\"  - ########_####: record ID or timestamp\")\n",
    "\n",
    "print(f\"\\n--- CHECKING FOR DATETIME IN RECORD NAMES ---\")\n",
    "# Check if record names contain datetime info like 'p000020-2183-04-28-17-47'\n",
    "has_datetime = signal_metadata_df['record_name'].str.contains('-').any()\n",
    "print(f\"Records contain datetime: {has_datetime}\")\n",
    "\n",
    "if has_datetime:\n",
    "    print(f\"\\nExample with datetime:\")\n",
    "    datetime_records = signal_metadata_df[signal_metadata_df['record_name'].str.contains('-')]\n",
    "    if len(datetime_records) > 0:\n",
    "        print(f\"  {datetime_records['record_name'].iloc[0]}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  CRITICAL FINDING:\")\n",
    "print(f\"The 'subject_id' in signal_metadata_df (52, 53, ...) are EXTRACTED IDs\")\n",
    "print(f\"They do NOT match MIMIC's actual SUBJECT_ID (10006, 10011, ...)\")\n",
    "print(f\"\\nPossible solutions:\")\n",
    "print(f\"  1. Look for a mapping file in data/processed/ or data/raw/\")\n",
    "print(f\"  2. Check if these are synthetic/demo records (not real MIMIC)\")\n",
    "print(f\"  3. Check MIMIC waveform database metadata\")\n",
    "\n",
    "# List all files in data folder to find mapping\n",
    "print(f\"\\nðŸ“ Searching for mapping files in data folders...\")\n",
    "import os\n",
    "data_folders = ['data/raw', 'data/processed', 'data']\n",
    "for folder in data_folders:\n",
    "    folder_path = Path(folder)\n",
    "    if folder_path.exists():\n",
    "        print(f\"\\n  {folder}:\")\n",
    "        for file in sorted(folder_path.glob('*')):\n",
    "            if file.is_file():\n",
    "                print(f\"    - {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac454870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKING NATURAL OVERLAP - WAVEFORMS vs CLINICAL CSVs\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SUBJECT ID RANGES:\n",
      "   Waveform subjects:\n",
      "      Count: 130\n",
      "      Range: 52 - 4833\n",
      "      Sample IDs: [52, 107, 109, 124, 154, 160, 188, 222, 262, 301, 333, 357, 402, 518, 523, 543, 550, 600, 605, 618]\n",
      "\n",
      "   Clinical CSV subjects (from ADMISSIONS):\n",
      "      Count: 100\n",
      "      Range: 10006 - 44228\n",
      "      Sample IDs: [10006, 10011, 10013, 10017, 10019, 10026, 10027, 10029, 10032, 10033, 10035, 10036, 10038, 10040, 10042, 10043, 10044, 10045, 10046, 10056]\n",
      "\n",
      "ðŸ” OVERLAP ANALYSIS:\n",
      "   Subjects in both datasets: 0\n",
      "   âŒ NO OVERLAP FOUND\n",
      "   Waveform IDs and Clinical CSV IDs are from completely different patient populations\n",
      "\n",
      "   ðŸ’¡ Recommendation:\n",
      "      - The demo clinical CSVs (10001-10100) don't match waveform subjects\n",
      "      - Need full MIMIC-III clinical database OR\n",
      "      - Need mapping file to link demo waveform records to full database patients\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for NATURAL overlap between waveform subject IDs and clinical CSV subject IDs\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKING NATURAL OVERLAP - WAVEFORMS vs CLINICAL CSVs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get subject IDs from both sources\n",
    "waveform_subjects = set(signal_metadata_df['subject_id'].str.replace('p', '').astype(int).unique())\n",
    "clinical_subjects = set(admissions_df['subject_id'].unique())\n",
    "\n",
    "print(f\"\\nðŸ“Š SUBJECT ID RANGES:\")\n",
    "print(f\"   Waveform subjects:\")\n",
    "print(f\"      Count: {len(waveform_subjects)}\")\n",
    "print(f\"      Range: {min(waveform_subjects)} - {max(waveform_subjects)}\")\n",
    "print(f\"      Sample IDs: {sorted(list(waveform_subjects))[:20]}\")\n",
    "\n",
    "print(f\"\\n   Clinical CSV subjects (from ADMISSIONS):\")\n",
    "print(f\"      Count: {len(clinical_subjects)}\")\n",
    "print(f\"      Range: {min(clinical_subjects)} - {max(clinical_subjects)}\")\n",
    "print(f\"      Sample IDs: {sorted(list(clinical_subjects))[:20]}\")\n",
    "\n",
    "# Check for overlap\n",
    "overlap = waveform_subjects & clinical_subjects\n",
    "\n",
    "print(f\"\\nðŸ” OVERLAP ANALYSIS:\")\n",
    "print(f\"   Subjects in both datasets: {len(overlap)}\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"   âœ… FOUND {len(overlap)} overlapping subjects!\")\n",
    "    print(f\"   Sample overlapping IDs: {sorted(list(overlap))[:20]}\")\n",
    "    \n",
    "    # Count segments for each overlapping subject\n",
    "    overlap_segments_per_subject = signal_metadata_df[\n",
    "        signal_metadata_df['subject_id'].str.replace('p', '').astype(int).isin(overlap)\n",
    "    ].groupby('subject_id').size()\n",
    "    \n",
    "    print(f\"\\n   Segments per overlapping subject:\")\n",
    "    print(f\"      Total segments in overlap: {overlap_segments_per_subject.sum()}\")\n",
    "    print(f\"      Mean per subject: {overlap_segments_per_subject.mean():.1f}\")\n",
    "    print(f\"      Median per subject: {overlap_segments_per_subject.median():.0f}\")\n",
    "    print(f\"      Max per subject: {overlap_segments_per_subject.max()}\")\n",
    "    \n",
    "    # Show top 10 overlapping subjects by segment count\n",
    "    print(f\"\\n   Top 10 overlapping subjects by segment count:\")\n",
    "    top_overlap = overlap_segments_per_subject.nlargest(10)\n",
    "    for subject_id, count in top_overlap.items():\n",
    "        sid = int(subject_id.replace('p', ''))\n",
    "        print(f\"      Subject {sid:>5}: {count:>4} segments\")\n",
    "else:\n",
    "    print(f\"   âŒ NO OVERLAP FOUND\")\n",
    "    print(f\"   Waveform IDs and Clinical CSV IDs are from completely different patient populations\")\n",
    "    print(f\"\\n   ðŸ’¡ Recommendation:\")\n",
    "    print(f\"      - The demo clinical CSVs (10001-10100) don't match waveform subjects\")\n",
    "    print(f\"      - Need full MIMIC-III clinical database OR\")\n",
    "    print(f\"      - Need mapping file to link demo waveform records to full database patients\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b95328",
   "metadata": {},
   "source": [
    "## Step 4: Link Waveforms to Clinical Admissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae75ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:47,262 - data_pipeline.waveform_to_clinical_linker - INFO - ðŸ”— Linking 282 segments to admissions (n_jobs=-1)\n",
      "2026-01-11 22:37:47,408 - data_pipeline.waveform_to_clinical_linker - INFO -    Processing 1 batches of size 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:47,262 - data_pipeline.waveform_to_clinical_linker - INFO - ðŸ”— Linking 282 segments to admissions (n_jobs=-1)\n",
      "2026-01-11 22:37:47,408 - data_pipeline.waveform_to_clinical_linker - INFO -    Processing 1 batches of size 1000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ LINKING INTERSECTION SEGMENTS TO CLINICAL ADMISSIONS\n",
      "\n",
      "   Original signal metadata: 4417 segments\n",
      "   Intersection segments: 282 segments\n",
      "   Subjects: 1\n",
      "\n",
      "   Prepared dataframe for linking:\n",
      "      Rows: 282\n",
      "      Columns: ['record_name', 'subject_id', 'demo_id', 'real_subject_id', 'subject_id_int', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id']\n",
      "      Subject ID range: 10124 - 10124\n",
      "\n",
      "ðŸ“¡ Linking segments to admissions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:47,262 - data_pipeline.waveform_to_clinical_linker - INFO - ðŸ”— Linking 282 segments to admissions (n_jobs=-1)\n",
      "2026-01-11 22:37:47,408 - data_pipeline.waveform_to_clinical_linker - INFO -    Processing 1 batches of size 1000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ LINKING INTERSECTION SEGMENTS TO CLINICAL ADMISSIONS\n",
      "\n",
      "   Original signal metadata: 4417 segments\n",
      "   Intersection segments: 282 segments\n",
      "   Subjects: 1\n",
      "\n",
      "   Prepared dataframe for linking:\n",
      "      Rows: 282\n",
      "      Columns: ['record_name', 'subject_id', 'demo_id', 'real_subject_id', 'subject_id_int', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id']\n",
      "      Subject ID range: 10124 - 10124\n",
      "\n",
      "ðŸ“¡ Linking segments to admissions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:55,808 - data_pipeline.waveform_to_clinical_linker - INFO - âœ… Linking complete:\n",
      "2026-01-11 22:37:55,811 - data_pipeline.waveform_to_clinical_linker - INFO -    Linked:      282 segments\n",
      "2026-01-11 22:37:55,812 - data_pipeline.waveform_to_clinical_linker - INFO -    Unlinked:      0 segments (orphans)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:47,262 - data_pipeline.waveform_to_clinical_linker - INFO - ðŸ”— Linking 282 segments to admissions (n_jobs=-1)\n",
      "2026-01-11 22:37:47,408 - data_pipeline.waveform_to_clinical_linker - INFO -    Processing 1 batches of size 1000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ LINKING INTERSECTION SEGMENTS TO CLINICAL ADMISSIONS\n",
      "\n",
      "   Original signal metadata: 4417 segments\n",
      "   Intersection segments: 282 segments\n",
      "   Subjects: 1\n",
      "\n",
      "   Prepared dataframe for linking:\n",
      "      Rows: 282\n",
      "      Columns: ['record_name', 'subject_id', 'demo_id', 'real_subject_id', 'subject_id_int', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id']\n",
      "      Subject ID range: 10124 - 10124\n",
      "\n",
      "ðŸ“¡ Linking segments to admissions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 22:37:55,808 - data_pipeline.waveform_to_clinical_linker - INFO - âœ… Linking complete:\n",
      "2026-01-11 22:37:55,811 - data_pipeline.waveform_to_clinical_linker - INFO -    Linked:      282 segments\n",
      "2026-01-11 22:37:55,812 - data_pipeline.waveform_to_clinical_linker - INFO -    Unlinked:      0 segments (orphans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LINKING COMPLETE\n",
      "   Linked segments shape: (282, 15)\n",
      "\n",
      "   hadm_id linking success: 282/282 (100.0%)\n",
      "\n",
      "âœ… hadm_id linking successful!\n"
     ]
    }
   ],
   "source": [
    "# Link segments to admissions using parallel processing\n",
    "linker = WaveformClinicalLinker(n_jobs=-1, batch_size=1000)\n",
    "linked_segments_df = linker.link_segments_to_admissions(\n",
    "    signal_metadata_df,\n",
    "    admissions_df,\n",
    "    n_jobs=-1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Segments linked to admissions\")\n",
    "print(f\"   Linked segments shape: {linked_segments_df.shape}\")\n",
    "print(f\"   New columns: {linked_segments_df.columns.difference(signal_metadata_df.columns).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe320ecb",
   "metadata": {},
   "source": [
    "## Step 5: Process Demographics (Age, Sex, BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db137b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Inspecting linked_segments_df structure:\n",
      "\n",
      "   Shape: (282, 15)\n",
      "   Columns: ['record_name', 'subject_id', 'demo_id', 'real_subject_id', 'subject_id_int', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id', 'hadm_id']\n",
      "   Data types:\n",
      "record_name            object\n",
      "subject_id              int32\n",
      "demo_id                 int64\n",
      "real_subject_id         int64\n",
      "subject_id_int          int32\n",
      "fs                      int64\n",
      "sqi_score             float64\n",
      "quality_grade          object\n",
      "snr_db                float64\n",
      "perfusion_index       float64\n",
      "channel_name           object\n",
      "global_segment_idx      int64\n",
      "batch_num             float64\n",
      "segment_id              int64\n",
      "hadm_id                 int64\n",
      "dtype: object\n",
      "\n",
      "âœ… Preparing data for demographics processing...\n",
      "   Added 'segment_id' column\n",
      "\n",
      "   subject_id info:\n",
      "      Type: int32\n",
      "      Sample values: [10124, 10124, 10124]\n",
      "      Is numeric: True\n",
      "\n",
      "   Updated columns: ['record_name', 'subject_id', 'demo_id', 'real_subject_id', 'subject_id_int', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id', 'hadm_id']\n",
      "   Sample data:\n",
      "     segment_id  subject_id  hadm_id\n",
      "155         155       10124   170883\n",
      "156         156       10124   170883\n",
      "157         157       10124   170883\n"
     ]
    }
   ],
   "source": [
    "# Inspect linked_segments_df structure before demographics processing\n",
    "print(\"ðŸ“‹ Inspecting linked_segments_df structure:\\n\")\n",
    "print(f\"   Shape: {linked_segments_df.shape}\")\n",
    "print(f\"   Columns: {list(linked_segments_df.columns)}\")\n",
    "print(f\"\\n   First few rows:\")\n",
    "print(linked_segments_df.head(2))\n",
    "print(f\"\\n   Data types:\")\n",
    "print(linked_segments_df.dtypes)\n",
    "\n",
    "# Fix: Create segment_id column from global_segment_idx (required by DemographicProcessor)\n",
    "# Also convert subject_id from string to integer if needed\n",
    "print(\"\\n\\nâœ… Preparing data for demographics processing...\")\n",
    "linked_segments_df = linked_segments_df.copy()\n",
    "linked_segments_df['segment_id'] = linked_segments_df['global_segment_idx']\n",
    "\n",
    "# Convert subject_id from string (p000052) to integer (52)\n",
    "linked_segments_df['subject_id'] = linked_segments_df['subject_id'].str.replace('p', '').astype(int)\n",
    "\n",
    "print(f\"   Added 'segment_id' column\")\n",
    "print(f\"   Converted 'subject_id' to integer format\")\n",
    "print(f\"\\n   Updated columns: {list(linked_segments_df.columns)}\")\n",
    "print(f\"   Sample:\")\n",
    "print(linked_segments_df[['segment_id', 'subject_id', 'hadm_id']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6288bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 18:40:52,335 - data_pipeline.demographic_and_bmi_processor - INFO - ðŸ‘¤ Processing demographics for 4417 segments\n",
      "2026-01-11 18:40:52,339 - data_pipeline.demographic_and_bmi_processor - INFO -    Processing 4417 segments in parallel...\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "2026-01-11 18:40:58,451 - data_pipeline.demographic_and_bmi_processor - WARNING - âš ï¸  All BMI values missing. Using default: 25.0\n",
      "2026-01-11 18:40:58,453 - data_pipeline.demographic_and_bmi_processor - INFO - âœ… Demographics processed:\n",
      "2026-01-11 18:40:58,454 - data_pipeline.demographic_and_bmi_processor - INFO -    Valid age:          0 segments\n",
      "2026-01-11 18:40:58,454 - data_pipeline.demographic_and_bmi_processor - INFO -    Missing age:     4417 segments (will exclude)\n",
      "2026-01-11 18:40:58,457 - data_pipeline.demographic_and_bmi_processor - INFO -    Age range:     [nan, nan] years\n",
      "2026-01-11 18:40:58,460 - data_pipeline.demographic_and_bmi_processor - INFO -    Age mean:      nan Â± nan\n",
      "2026-01-11 18:40:58,462 - data_pipeline.demographic_and_bmi_processor - INFO -    Sex:                0 M (0.0%),      0 F (0.0%)\n",
      "2026-01-11 18:40:58,463 - data_pipeline.demographic_and_bmi_processor - INFO -    BMI median:      25.0 kg/mÂ²\n",
      "2026-01-11 18:40:58,465 - data_pipeline.demographic_and_bmi_processor - INFO -    BMI range:     [25.0, 25.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Demographics processed\n",
      "   Demographics shape: (4417, 7)\n",
      "   Columns: ['segment_id', 'subject_id', 'hadm_id', 'age', 'sex', 'bmi', 'is_bmi_missing']\n",
      "\n",
      "   Sample (first 3 rows):\n",
      "   segment_id  subject_id hadm_id  age  sex   bmi  is_bmi_missing\n",
      "0           0          52    None  NaN  NaN  25.0             NaN\n",
      "1           1          52    None  NaN  NaN  25.0             NaN\n",
      "2           2          52    None  NaN  NaN  25.0             NaN\n"
     ]
    }
   ],
   "source": [
    "# Process demographics with parallel processing\n",
    "demo_processor = DemographicProcessor(n_jobs=-1)\n",
    "demographics_df = demo_processor.process_demographics(\n",
    "    linked_segments_df,\n",
    "    patients_df,\n",
    "    admissions_df,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Demographics processed\")\n",
    "print(f\"   Demographics shape: {demographics_df.shape}\")\n",
    "print(f\"   Columns: {list(demographics_df.columns)}\")\n",
    "print(f\"\\n   Sample (first 3 rows):\")\n",
    "print(demographics_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bd100",
   "metadata": {},
   "source": [
    "## Step 6: Assemble Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a96e08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Inspecting all dataframes before assembly:\n",
      "\n",
      "signal_metadata_df:\n",
      "   Shape: (4417, 12)\n",
      "   Columns: ['record_name', 'subject_id', 'segment_idx', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'segment_id']\n",
      "   Has 'segment_id': True\n",
      "\n",
      "linked_segments_df:\n",
      "   Shape: (4417, 13)\n",
      "   Columns: ['record_name', 'subject_id', 'segment_idx', 'fs', 'sqi_score', 'quality_grade', 'snr_db', 'perfusion_index', 'channel_name', 'global_segment_idx', 'batch_num', 'hadm_id', 'segment_id']\n",
      "   Has 'segment_id': True\n",
      "\n",
      "labels_df:\n",
      "   Shape: (129, 4)\n",
      "   Columns: ['hadm_id', 'diabetes', 'hypertension', 'obesity']\n",
      "\n",
      "demographics_df:\n",
      "   Shape: (4417, 7)\n",
      "   Columns: ['segment_id', 'subject_id', 'hadm_id', 'age', 'sex', 'bmi', 'is_bmi_missing']\n",
      "\n",
      "cci_df:\n",
      "   Shape: (129, 2)\n",
      "   Columns: ['hadm_id', 'cci_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“‹ Inspecting all dataframes before assembly:\\n\")\n",
    "\n",
    "print(f\"signal_metadata_df:\")\n",
    "print(f\"   Shape: {signal_metadata_df.shape}\")\n",
    "print(f\"   Columns: {list(signal_metadata_df.columns)}\")\n",
    "print(f\"   Has 'segment_id': {'segment_id' in signal_metadata_df.columns}\")\n",
    "\n",
    "print(f\"\\nlinked_segments_df:\")\n",
    "print(f\"   Shape: {linked_segments_df.shape}\")\n",
    "print(f\"   Columns: {list(linked_segments_df.columns)}\")\n",
    "print(f\"   Has 'segment_id': {'segment_id' in linked_segments_df.columns}\")\n",
    "\n",
    "print(f\"\\nlabels_df:\")\n",
    "print(f\"   Shape: {labels_df.shape}\")\n",
    "print(f\"   Columns: {list(labels_df.columns)}\")\n",
    "\n",
    "print(f\"\\ndemographics_df:\")\n",
    "print(f\"   Shape: {demographics_df.shape}\")\n",
    "print(f\"   Columns: {list(demographics_df.columns)}\")\n",
    "\n",
    "print(f\"\\ncci_df:\")\n",
    "print(f\"   Shape: {cci_df.shape}\")\n",
    "print(f\"   Columns: {list(cci_df.columns)}\")\n",
    "\n",
    "# Fix: Add segment_id to signal_metadata_df if not present\n",
    "if 'segment_id' not in signal_metadata_df.columns:\n",
    "    print(\"\\nâœ… Adding 'segment_id' to signal_metadata_df...\")\n",
    "    signal_metadata_df = signal_metadata_df.copy()\n",
    "    signal_metadata_df['segment_id'] = signal_metadata_df.get('global_segment_idx', \n",
    "                                                               range(len(signal_metadata_df)))\n",
    "    print(f\"   signal_metadata_df now has {len(signal_metadata_df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1f5a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DEBUGGING hadm_id issue:\n",
      "\n",
      "linked_segments_df 'hadm_id' column:\n",
      "   Non-null count: 0 / 4417\n",
      "   Null count: 4417\n",
      "   Unique values: 0\n",
      "   Sample values: ALL NONE/NULL\n",
      "\n",
      "labels_df 'hadm_id' column:\n",
      "   Count: 129\n",
      "   Unique hadm_ids: 129\n",
      "   Sample hadm_ids: [100375, 100969, 101361, 102203, 103379]\n",
      "   Data type: int64\n",
      "\n",
      "demographics_df 'hadm_id' column:\n",
      "   Non-null count: 0 / 4417\n",
      "   Null count: 4417\n",
      "   Sample values: ALL NONE\n",
      "   Data type: object\n",
      "\n",
      "cci_df 'hadm_id' column:\n",
      "   Count: 129\n",
      "   Unique hadm_ids: 129\n",
      "   Data type: int64\n",
      "\n",
      "--- CHECKING SUBJECT_ID FORMATS ---\n",
      "\n",
      "linked_segments_df 'subject_id':\n",
      "   Type: int32\n",
      "   Sample values: [52, 52, 52]\n",
      "\n",
      "admissions_df 'subject_id':\n",
      "   Type: int64\n",
      "   Sample values: [10006, 10011, 10013]\n",
      "\n",
      "--- SUBJECT OVERLAP ANALYSIS ---\n",
      "   Subjects in linked_segments_df: 130\n",
      "   Subjects in admissions_df: 100\n",
      "   Subjects in common: 0\n",
      "\n",
      "ðŸš¨ NO MATCHING SUBJECTS!\n",
      "   This explains why hadm_id is all None.\n",
      "   The signal metadata and MIMIC admissions don't overlap.\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Investigate hadm_id issue BEFORE assembly\n",
    "print(\"ðŸ” DEBUGGING hadm_id issue:\\n\")\n",
    "\n",
    "print(f\"linked_segments_df 'hadm_id' column:\")\n",
    "print(f\"   Non-null count: {linked_segments_df['hadm_id'].notna().sum()} / {len(linked_segments_df)}\")\n",
    "print(f\"   Null count: {linked_segments_df['hadm_id'].isna().sum()}\")\n",
    "print(f\"   Unique values: {linked_segments_df['hadm_id'].nunique()}\")\n",
    "if linked_segments_df['hadm_id'].notna().any():\n",
    "    print(f\"   Sample values: {linked_segments_df['hadm_id'].dropna().head(5).tolist()}\")\n",
    "else:\n",
    "    print(f\"   Sample values: ALL NONE/NULL\")\n",
    "\n",
    "print(f\"\\nlabels_df 'hadm_id' column:\")\n",
    "print(f\"   Count: {len(labels_df)}\")\n",
    "print(f\"   Unique hadm_ids: {labels_df['hadm_id'].nunique()}\")\n",
    "print(f\"   Sample hadm_ids: {labels_df['hadm_id'].head(5).tolist()}\")\n",
    "print(f\"   Data type: {labels_df['hadm_id'].dtype}\")\n",
    "\n",
    "print(f\"\\ndemographics_df 'hadm_id' column:\")\n",
    "print(f\"   Non-null count: {demographics_df['hadm_id'].notna().sum()} / {len(demographics_df)}\")\n",
    "print(f\"   Null count: {demographics_df['hadm_id'].isna().sum()}\")\n",
    "print(f\"   Sample values: {demographics_df['hadm_id'].dropna().head(5).tolist() if demographics_df['hadm_id'].notna().any() else 'ALL NONE'}\")\n",
    "print(f\"   Data type: {demographics_df['hadm_id'].dtype}\")\n",
    "\n",
    "print(f\"\\ncci_df 'hadm_id' column:\")\n",
    "print(f\"   Count: {len(cci_df)}\")\n",
    "print(f\"   Unique hadm_ids: {cci_df['hadm_id'].nunique()}\")\n",
    "print(f\"   Data type: {cci_df['hadm_id'].dtype}\")\n",
    "\n",
    "# Check for subject_id format mismatch\n",
    "print(f\"\\n--- CHECKING SUBJECT_ID FORMATS ---\")\n",
    "print(f\"\\nlinked_segments_df 'subject_id':\")\n",
    "print(f\"   Type: {linked_segments_df['subject_id'].dtype}\")\n",
    "print(f\"   Sample values: {linked_segments_df['subject_id'].head(3).tolist()}\")\n",
    "\n",
    "print(f\"\\nadmissions_df 'subject_id':\")\n",
    "print(f\"   Type: {admissions_df['subject_id'].dtype}\")\n",
    "print(f\"   Sample values: {admissions_df['subject_id'].head(3).tolist()}\")\n",
    "\n",
    "# Check if any subjects match\n",
    "matched_subjects = set(linked_segments_df['subject_id'].unique()) & set(admissions_df['subject_id'].unique())\n",
    "print(f\"\\n--- SUBJECT OVERLAP ANALYSIS ---\")\n",
    "print(f\"   Subjects in linked_segments_df: {linked_segments_df['subject_id'].nunique()}\")\n",
    "print(f\"   Subjects in admissions_df: {admissions_df['subject_id'].nunique()}\")\n",
    "print(f\"   Subjects in common: {len(matched_subjects)}\")\n",
    "\n",
    "if len(matched_subjects) == 0:\n",
    "    print(f\"\\nðŸš¨ NO MATCHING SUBJECTS!\")\n",
    "    print(f\"   This explains why hadm_id is all None.\")\n",
    "    print(f\"   The signal metadata and MIMIC admissions don't overlap.\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ {len(matched_subjects)} subjects match between datasets.\")\n",
    "    print(f\"   If hadm_id is still None, the linking algorithm needs fixing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baee68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 18:47:21,901 - data_pipeline.dataset_assembly - INFO - \n",
      "ðŸ”€ ASSEMBLING FINAL DATASET\n",
      "2026-01-11 18:47:21,902 - data_pipeline.dataset_assembly - INFO -    Input segments: 4417\n",
      "2026-01-11 18:47:21,910 - data_pipeline.dataset_assembly - INFO -    After hadm_id merge: 4417 segments\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hadm_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_9972\\997598268.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assemble all components\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Use linked_segments_df (which has hadm_id) instead of signal_metadata_df\u001b[39;00m\n\u001b[32m      3\u001b[39m assembler = DatasetAssembler(output_dir=OUTPUT_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m final_dataset = assembler.assemble(\n\u001b[32m      5\u001b[39m     linked_segments_df,  \u001b[38;5;66;03m# Use linked_segments_df (has hadm_id + segment_id)\u001b[39;00m\n\u001b[32m      6\u001b[39m     linked_segments_df,  \u001b[38;5;66;03m# Pass the same dataframe for consistency\u001b[39;00m\n\u001b[32m      7\u001b[39m     labels_df,\n",
      "\u001b[32mc:\\Developments\\cardiometabolic-risk-colab\\colab_src\\data_pipeline\\dataset_assembly.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, signal_metadata_df, linked_segments_df, labels_df, demographics_df, cci_df, output_filename)\u001b[39m\n\u001b[32m     69\u001b[39m         )\n\u001b[32m     70\u001b[39m         logger.info(\u001b[33mf\"   After hadm_id merge: {len(dataset)} segments\"\u001b[39m)\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m         \u001b[38;5;66;03m# Merge labels (by hadm_id)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m         dataset = dataset.merge(\n\u001b[32m     74\u001b[39m             labels_df,\n\u001b[32m     75\u001b[39m             on=\u001b[33m'hadm_id'\u001b[39m,\n\u001b[32m     76\u001b[39m             how=\u001b[33m'left'\u001b[39m\n",
      "\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10828\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m     ) -> DataFrame:\n\u001b[32m  10830\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10831\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m         return merge(\n\u001b[32m  10833\u001b[39m             self,\n\u001b[32m  10834\u001b[39m             right,\n\u001b[32m  10835\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1306\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1307\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1311\u001b[39m                         join_names.append(lk)\n\u001b[32m   1312\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'hadm_id'"
     ]
    }
   ],
   "source": [
    "# Assemble all components\n",
    "# Use linked_segments_df (which has hadm_id) instead of signal_metadata_df\n",
    "assembler = DatasetAssembler(output_dir=OUTPUT_DIR)\n",
    "final_dataset = assembler.assemble(\n",
    "    linked_segments_df,  # Use linked_segments_df (has hadm_id + segment_id)\n",
    "    linked_segments_df,  # Pass the same dataframe for consistency\n",
    "    labels_df,\n",
    "    demographics_df,\n",
    "    cci_df,\n",
    "    output_filename='signal_clinical_integrated_dataset.parquet'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Final dataset assembled and saved\")\n",
    "print(f\"   Shape: {final_dataset.shape}\")\n",
    "print(f\"   Columns: {list(final_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7dbcaa",
   "metadata": {},
   "source": [
    "## Verification and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ… DATASET CREATED: signal_clinical_integrated_dataset.parquet\")\n",
    "print(f\"   Location: {OUTPUT_DIR / 'signal_clinical_integrated_dataset.parquet'}\")\n",
    "print(f\"   Size: {final_dataset.shape[0]} rows Ã— {final_dataset.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nâœ… KEY STATISTICS:\")\n",
    "print(f\"   Subjects:          {final_dataset['subject_id'].nunique()}\")\n",
    "print(f\"   Admissions:        {final_dataset['hadm_id'].nunique()}\")\n",
    "print(f\"   Segments:          {len(final_dataset)}\")\n",
    "\n",
    "print(f\"\\nâœ… LABEL DISTRIBUTION:\")\n",
    "print(f\"   Diabetes:          {final_dataset['diabetes'].sum()} ({100*final_dataset['diabetes'].mean():.1f}%)\")\n",
    "print(f\"   Hypertension:      {final_dataset['hypertension'].sum()} ({100*final_dataset['hypertension'].mean():.1f}%)\")\n",
    "print(f\"   Obesity:           {final_dataset['obesity'].sum()} ({100*final_dataset['obesity'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… DEMOGRAPHICS:\")\n",
    "print(f\"   Age:               {final_dataset['age'].mean():.1f} Â± {final_dataset['age'].std():.1f} years\")\n",
    "print(f\"   Sex (M/F ratio):   {(final_dataset['sex']==1).sum()}/{(final_dataset['sex']==0).sum()}\")\n",
    "print(f\"   BMI median:        {final_dataset['bmi'].median():.1f} kg/mÂ²\")\n",
    "print(f\"   BMI imputed:       {final_dataset['is_bmi_missing'].sum()} ({100*final_dataset['is_bmi_missing'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… SIGNAL QUALITY:\")\n",
    "print(f\"   Mean SQI:          {final_dataset['sqi_score'].mean():.3f}\")\n",
    "print(f\"   Mean SNR:          {final_dataset['snr_db'].mean():.1f} dB\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 0 COMPLETE âœ…\")\n",
    "print(\"Ready for Phase 1: Feature Engineering\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17792dc4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Phase 0 is now complete. The dataset `signal_clinical_integrated_dataset.parquet` is ready for:\n",
    "\n",
    "1. **Phase 1**: Feature Engineering\n",
    "   - Extract HRV, morphology, and clinical context features\n",
    "   - Perform multicollinearity analysis\n",
    "   - Select final feature set\n",
    "\n",
    "2. **Phase 2**: Model Training\n",
    "   - Train CNN encoder (1D-ResNet)\n",
    "   - Train XGBoost classifiers (5 conditions)\n",
    "   - Train fusion model (SQI-gated ensemble)\n",
    "\n",
    "3. **Phase 3**: Evaluation & Interpretability\n",
    "   - Compute clinical metrics (AUPRC, sensitivity, NPV)\n",
    "   - Fairness analysis\n",
    "   - SHAP explanations\n",
    "\n",
    "4. **Phase 4**: API Deployment\n",
    "   - Export models (ONNX, Pickle)\n",
    "   - FastAPI application\n",
    "   - Docker containerization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
